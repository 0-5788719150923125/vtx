% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{evans2018pragmatic,
  title={A pragmatic introduction to secure multi-party computation},
  author={Evans, David and Kolesnikov, Vladimir and Rosulek, Mike and others},
  journal={Foundations and Trends in Privacy and Security},
  volume={2},
  number={2-3},
  pages={70--246},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@misc{nvidia-privacy,
  title={NVIDIA Confidential Computing},
  author={NVIDIA},
  year={2022},
  howpublished = {\url{https://www.nvidia.com/en-in/data-center/solutions/confidential-computing/}}
}

@article{ethereum,
  title={Ethereum: A next-generation smart contract and decentralized application platform},
  author={Buterin, Vitalik},
  year={2014},
  howpublished = {\url{https://ethereum.org/en/whitepaper/}}
}

@article{near,
  title={Nightshade: Near protocol sharding design},
  author={Skidanov, Alex and Polosukhin, Illia},
  howpublished = {\url{https://nearprotocol.com/downloads/Nightshade.pdf}},
  year={2019}
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@misc{gehrmann2022gemv2,
    title={GEMv2: Multilingual NLG Benchmarking in a Single Line of Code},
    author={Sebastian Gehrmann and Abhik Bhattacharjee and Abinaya Mahendiran and Alex Wang and Alexandros Papangelis and Aman Madaan and Angelina McMillan-Major and Anna Shvets and Ashish Upadhyay and Bingsheng Yao and Bryan Wilie and Chandra Bhagavatula and Chaobin You and Craig Thomson and Cristina Garbacea and Dakuo Wang and Daniel Deutsch and Deyi Xiong and Di Jin and Dimitra Gkatzia and Dragomir Radev and Elizabeth Clark and Esin Durmus and Faisal Ladhak and Filip Ginter and Genta Indra Winata and Hendrik Strobelt and Hiroaki Hayashi and Jekaterina Novikova and Jenna Kanerva and Jenny Chim and Jiawei Zhou and Jordan Clive and Joshua Maynez and João Sedoc and Juraj Juraska and Kaustubh Dhole and Khyathi Raghavi Chandu and Laura Perez-Beltrachini and Leonardo F. R. Ribeiro and Lewis Tunstall and Li Zhang and Mahima Pushkarna and Mathias Creutz and Michael White and Mihir Sanjay Kale and Moussa Kamal Eddine and Nico Daheim and Nishant Subramani and Ondrej Dusek and Paul Pu Liang and Pawan Sasanka Ammanamanchi and Qi Zhu and Ratish Puduppully and Reno Kriz and Rifat Shahriyar and Ronald Cardenas and Saad Mahamood and Salomey Osei and Samuel Cahyawijaya and Sanja Štajner and Sebastien Montella and Shailza and Shailza Jolly and Simon Mille and Tahmid Hasan and Tianhao Shen and Tosin Adewumi and Vikas Raunak and Vipul Raheja and Vitaly Nikolaev and Vivian Tsai and Yacine Jernite and Ying Xu and Yisi Sang and Yixin Liu and Yufang Hou},
    year={2022},
    eprint={2206.11249},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{dynabench,
  doi = {10.48550/ARXIV.2104.14337},
  
  url = {https://arxiv.org/abs/2104.14337},
  
  author = {Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and Ma, Zhiyi and Thrush, Tristan and Riedel, Sebastian and Waseem, Zeerak and Stenetorp, Pontus and Jia, Robin and Bansal, Mohit and Potts, Christopher and Williams, Adina},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dynabench: Rethinking Benchmarking in NLP},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% The usual stuff %%%%%%%%
@misc{TURN,
  title        = {{Traversal Using Relays around NAT (TURN): Relay Extensions to Session Traversal Utilities for NAT (STUN)}},
  author       = {Tirumaleswar Reddy.K and Alan Johnston and Philip Matthews and Jonathan Rosenberg},
  year         = 2020,
  month        = feb,
  publisher    = {RFC Editor},
  series       = {Request for Comments},
  number       = 8656,
  doi          = {10.17487/RFC8656},
  url          = {https://rfc-editor.org/rfc/rfc8656.txt},
  howpublished = {RFC 8656},
  pagetotal    = 79
}

@article{ryabinin2021swarm,
  title={SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient},
  author={Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  year={2021}
}


@article{ma2019adequacy,
  title        = {On the adequacy of untuned warmup for adaptive optimization},
  author       = {Ma, Jerry and Yarats, Denis},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1910.04209},
  volume       = 7
}
@inproceedings{MLSYS2020_96da2f59,
  title        = {BPPSA: Scaling Back-propagation by Parallel Scan Algorithm},
  author       = {Wang, Shang and Bai, Yifan and Pekhimenko, Gennady},
  year         = 2020,
  booktitle    = {Proceedings of Machine Learning and Systems},
  volume       = 2,
  pages        = {451--469},
  url          = {https://proceedings.mlsys.org/paper/2020/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf},
  editor       = {I. Dhillon and D. Papailiopoulos and V. Sze}
}
@article{STUN,
  title        = {STUN—Simple traversal of user datagram protocol (UDP) through network address translators (NATs)},
  author       = {Weinberger, J. and Huitema, C. and Mahy, R.},
  year         = 2003,
  month        = {01},
  journal      = {IETF RFC 3489},
  pages        = {}
}
@inproceedings{NATFord,
  title        = {Peer-to-Peer Communication Across Network Address Translators},
  author       = {B. Ford and P. Srisuresh and Dan Kegel},
  year         = 2005,
  booktitle    = {USENIX Annual Technical Conference, General Track}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Misc from intro section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{protein2vec,
  title        = {Protein2Vec: Aligning Multiple PPI Networks with Representation Learning},
  author       = {Gao, Jianliang and Tian, Ling and Lv, Tengfei and Wang, Jianxin and Song, Bo and Hu, Xiaohua},
  year         = 2019,
  month        = {08},
  journal      = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume       = {PP},
  pages        = {1--1},
  doi          = {10.1109/TCBB.2019.2937771}
}
@misc{bertologybiology,
  title        = {BERTology Meets Biology: Interpreting Attention in Protein Language Models},
  author       = {Jesse Vig and Ali Madani and Lav R. Varshney and Caiming Xiong and Richard Socher and Nazneen Fatema Rajani},
  year         = 2020,
  url          = {https://arxiv.org/abs/2006.15222},
  eprint       = {2006.15222},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@misc{torrent,
  title        = {{The BitTorrent Protocol Specification}},
  author       = {Bram Cohen},
  year         = 2008,
  howpublished = {\url{http://www.bittorrent.org/beps/bep_0003.html}}
}
,
   note = {Accessed: 2021-10-05}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Federated Learning section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{FedLearningOriginal,
  title        = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author       = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  year         = 2017,
  booktitle    = {Artificial Intelligence and Statistics},
  pages        = {1273--1282}
}
@inproceedings{FedLearningAtScale,
  title        = {Towards Federated Learning at Scale: System Design},
  author       = {K. A. Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloé M Kiddon and Jakub Konečný and Stefano Mazzocchi and Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
  year         = 2019,
  booktitle    = {SysML 2019},
  url          = {https://arxiv.org/abs/1902.01046},
  note         = {To appear}
}
@inproceedings{PracticalSecureAggregation,
  title        = {Practical secure aggregation for privacy-preserving machine learning},
  author       = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  year         = 2017,
  booktitle    = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages        = {1175--1191}
}
@article{FedLearningDiffPrivacy,
  title        = {Federated Learning with Differential Privacy: Algorithms and Performance Analysis},
  author       = {Kang Wei and Jun Li and Ming Ding and Chuan Ma and Howard H. Yang and Farhad Farokhi and Shi Jin and Tony Q. S. Quek and H. Vincent Poor},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1911.00222},
  url          = {http://arxiv.org/abs/1911.00222},
  archiveprefix = {arXiv},
  eprint       = {1911.00222},
  timestamp    = {Thu, 14 Jan 2021 15:41:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-00222.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{FedLearningAdvancesProblems,
  title        = {Advances and open problems in federated learning},
  author       = {Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1912.04977}
}
@misc{FedLearningDecentralized,
  title        = {Decentralized Federated Learning Preserves Model and Data Privacy},
  author       = {Thorsten Wittkopp and Alexander Acker},
  year         = 2021,
  eprint       = {2102.00880},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Federated Learning section end %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Volunteer computing section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@manual{folding_covid,
  title        = {Folding@home update on SARS-COV-2 (10 MAR 2020)},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://foldingathome.org/covid19/}}
}
@manual{folding_timeline,
  title        = {Folding@home project timeline},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://foldingathome.org/project-timeline}}
}
% https://folding.typepad.com/news/2007/09/crossing-the-pe.html
@article{folding_petaflop,
  title        = {Folding research recruits unconventional help},
  author       = {Gross, Michael},
  year         = 2012,
  month        = {01},
  journal      = {Current biology : CB},
  volume       = 22,
  pages        = {R35--8},
  doi          = {10.1016/j.cub.2012.01.008}
}
@manual{folding_exaflop_1,
  title        = {Folding@home active CPU and GPU by OS},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://archive.md/20200412111010/https://stats.foldingathome.org/os}}
}
@manual{folding_exaflop_2,
  title        = {Folding@home gets 1.5+ Exaflops to Fight COVID-19},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://blogs.nvidia.com/blog/2020/04/01/foldingathome-exaflop-coronavirus/}}
}
@article{folding_ps3,
  title        = {Accelerating Molecular Dynamics Simulations on PlayStation 3 Platform Using Virtual-GRAPE Programming Model},
  author       = {Narumi, Tetsu and Kameoka, Shun and Taiji, Makoto and Yasuoka, Kenji},
  year         = 2008,
  month        = {01},
  journal      = {SIAM J. Scientific Computing},
  volume       = 30,
  pages        = {3108--3125},
  doi          = {10.1137/070692054}
}
@article{larson_crowd,
  title        = {Folding@Home and Genome@Home: Using distributed computing to tackle previously intractable problems in computational biology},
  author       = {Larson, Stefan and Snow, Christopher and Shirts, Michael and Pande, Vijay},
  year         = 2009,
  month        = {02},
  journal      = {arXiv},
  pages        = {}
}
@inproceedings{anderson2004boinc,
  title        = {Boinc: A system for public-resource computing and storage},
  author       = {Anderson, David P},
  year         = 2004,
  booktitle    = {Fifth IEEE/ACM international workshop on grid computing},
  pages        = {4--10},
  organization = {IEEE}
}
@article{lhc_at_home,
  title        = {LHC@Home: a BOINC-based volunteer computing infrastructure for physics studies at CERN},
  author       = {Barranco, Javier and Cai, Yunhi and Cameron, David and Crouch, Matthew and De Maria, Riccardo and Field, Laurence and Giovannozzi, M. and Hermes, Pascal and Høimyr, Nils and Kaltchev, Dobrin and Karastathis, Nikos and Luzzi, Cinzia and Maclean, Ewen and Mcintosh, Eric and Mereghetti, Alessio and Molson, James and Nosochkov, Yuri and Pieloni, Tatiana and Reid, Ivan and Zacharov, Igor},
  year         = 2017,
  month        = 12,
  journal      = {Open Engineering},
  volume       = 7,
  pages        = {},
  doi          = {10.1515/eng-2017-0042}
}
@article{seti_at_home,
  title        = {SETI@home: An Experiment in Public-Resource Computing},
  author       = {Anderson, David and Cobb, Jeff and Korpela, Eric and Lebofsky, Matt and Werthimer, Dan},
  year         = 2002,
  month        = 11,
  journal      = {Commun. ACM},
  volume       = 45,
  pages        = {56--61},
  doi          = {10.1145/581571.581573}
}
@misc{clemens2021mlds,
  title        = {MLDS: A Dataset for Weight-Space Analysis of Neural Networks},
  author       = {John Clemens},
  year         = 2021,
  eprint       = {2104.10555},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@manual{vastai,
  title        = {vast.ai},
  note         = {\url{https://vast.ai}}
}
@article{volunteer_dl_async,
  title        = {A hybrid GPU cluster and volunteer computing platform for scalable deep learning},
  author       = {Kijsipongse, Ekasit and Piyatumrong, Apivadee and U-ruekolan, Suriya},
  year         = 2018,
  month        = {04},
  journal      = {The Journal of Supercomputing},
  pages        = {},
  doi          = {10.1007/s11227-018-2375-9}
}
@misc{eydle,
  title        = {Distributed Deep Learning Using Volunteer Computing-Like Paradigm},
  author       = {Medha Atre and Birendra Jha and Ashwini Rao},
  year         = 2021,
  eprint       = {2103.08894},
  archiveprefix = {arXiv},
  primaryclass = {cs.DC}
}
@article{qmc_at_home,
  title        = {{QMCPACK}: an open sourceab initioquantum Monte Carlo package for the electronic structure of atoms, molecules and solids},
  author       = {Jeongnim Kim and Andrew D Baczewski and Todd D Beaudet and Anouar Benali and M Chandler Bennett and Mark A Berrill and Nick S Blunt and Edgar Josu{\'{e}} Landinez Borda and Michele Casula and David M Ceperley and Simone Chiesa and Bryan K Clark and Raymond C Clay and Kris T Delaney and Mark Dewing and Kenneth P Esler and Hongxia Hao and Olle Heinonen and Paul R C Kent and Jaron T Krogel and Ilkka Kylänpää and Ying Wai Li and M Graham Lopez and Ye Luo and Fionn D Malone and Richard M Martin and Amrita Mathuriya and Jeremy McMinis and Cody A Melton and Lubos Mitas and Miguel A Morales and Eric Neuscamman and William D Parker and Sergio D Pineda Flores and Nichols A Romero and Brenda M Rubenstein and Jacqueline A R Shea and Hyeondeok Shin and Luke Shulenburger and Andreas F Tillack and Joshua P Townsend and Norm M Tubman and Brett Van Der Goetz and Jordan E Vincent and D ChangMo Yang and Yubo Yang and Shuai Zhang and Luning Zhao},
  year         = 2018,
  month        = {apr},
  journal      = {Journal of Physics: Condensed Matter},
  publisher    = {{IOP} Publishing},
  volume       = 30,
  number       = 19,
  pages        = 195901,
  doi          = {10.1088/1361-648x/aab9c3},
  url          = {https://doi.org/10.1088/1361-648x/aab9c3}
}
@inproceedings{hivemind_dmoe,
  title        = {Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts},
  author       = {Ryabinin, Max and Gusev, Anton},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 33,
  pages        = {3659--3672},
  url          = {https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf},
  editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}
@article{dedloc,
  title        = {Distributed Deep Learning in Open Collaborations},
  author       = {Michael Diskin and Alexey Bukhtiyarov and Max Ryabinin and Lucile Saulnier and Quentin Lhoest and Anton Sinitsin and Dmitriy Popov and Dmitry Pyrkin and Maxim Kashirin and Alexander Borzunov and Albert Villanova del Moral and Denis Mazur and Ilia Kobelev and Yacine Jernite and Thomas Wolf and Gennady Pekhimenko},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2106.10207},
  url          = {https://arxiv.org/abs/2106.10207},
  eprinttype   = {arXiv},
  eprint       = {2106.10207},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-10207.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Volunteer computing section end %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{Trask2015ModelingOI,
  title        = {Modeling Order in Neural Word Embeddings at Scale},
  author       = {Andrew Trask and David Gilmore and Matthew Russell},
  year         = 2015,
  booktitle    = {ICML}
}
@inproceedings{gross_folding,
  title        = {Folding research recruits unconventional help},
  author       = {Michael Gross},
  year         = 2012,
  booktitle    = {Current Biology. 22 (2): R35–R38},
  doi          = {10.1016/j.cub.2012.01.008},
  pmid         = {PMID 22389910}
}
@inproceedings{imagenet_cvpr09,
  title        = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  author       = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  year         = 2009,
  booktitle    = {CVPR09},
  bibsource    = {http://www.image-net.org/papers/imagenet_cvpr09.bib}
}
@incollection{alexnet,
  title        = {ImageNet Classification with Deep Convolutional Neural Networks},
  author       = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  year         = 2012,
  booktitle    = {Advances in Neural Information Processing Systems 25},
  publisher    = {Curran Associates, Inc.},
  pages        = {1097--1105},
  url          = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}
@article{resnet,
  title        = {Deep Residual Learning for Image Recognition},
  author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year         = 2015,
  journal      = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {770--778}
}
@inproceedings{huang2019gpipe,
  title        = {Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author       = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {103--112}
}
@article{megatron2,
  title        = {Efficient Large-Scale Language Model Training on GPU Clusters},
  author       = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  year         = 2021,
  journal      = {arXiv preprint arXiv:2104.04473}
}
@article{ernie3,
  title        = {{ERNIE} 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},
  author       = {Yu Sun and Shuohuan Wang and Shikun Feng and Siyu Ding and Chao Pang and Junyuan Shang and Jiaxiang Liu and Xuyi Chen and Yanbin Zhao and Yuxiang Lu and Weixin Liu and Zhihua Wu and Weibao Gong and Jianzhong Liang and Zhizhou Shang and Peng Sun and Wei Liu and Xuan Ouyang and Dianhai Yu and Hao Tian and Hua Wu and Haifeng Wang},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2107.02137},
  url          = {https://arxiv.org/abs/2107.02137},
  eprinttype   = {arXiv},
  eprint       = {2107.02137},
  timestamp    = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-02137.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{retro,
  doi = {10.48550/ARXIV.2112.04426},
  
  url = {https://arxiv.org/abs/2112.04426},
  
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Improving language models by retrieving from trillions of tokens},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{fedus2021switch,
  title        = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author       = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year         = 2021,
  journal      = {arXiv preprint arXiv:2101.03961}
}
@article{jft300data,
  title        = {Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
  author       = {Baoyuan Wu and Weidong Chen and Yanbo Fan and Yong Zhang and Jinlong Hou and Jie Liu and Tong Zhang},
  year         = 2019,
  journal      = {IEEE Access},
  volume       = 7,
  pages        = {172683--172693}
}
@article{kolesnikovlarge,
  title        = {Large Scale Learning of General Visual Representations for Transfer},
  author       = {Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1912.11370},
  url          = {http://arxiv.org/abs/1912.11370},
  archiveprefix = {arXiv},
  eprint       = {1912.11370},
  timestamp    = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1912-11370},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bert,
  title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author       = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year         = 2019,
  booktitle    = {NAACL-HLT}
}
@article{roberta,
  title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1907.11692}
}
@article{shoeybi2019megatron,
  title        = {Megatron-lm: Training multi-billion parameter language models using gpu model parallelism},
  author       = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1909.08053}
}
@article{brown2020language,
  title        = {Language Models are Few-Shot Learners},
  author       = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2005.14165}
}
@article{mpich_rabenseifner,
  title        = {Optimization of Collective Communication Operations in MPICH},
  author       = {Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
  year         = 2005,
  month        = feb,
  journal      = {Int. J. High Perform. Comput. Appl.},
  publisher    = {Sage Publications, Inc.},
  address      = {USA},
  volume       = 19,
  number       = 1,
  pages        = {49–66},
  doi          = {10.1177/1094342005051521},
  issn         = {1094-3420},
  url          = {https://doi.org/10.1177/1094342005051521},
  issue_date   = {February  2005},
  abstract     = {We describe our work on improving the performance of collective communication operations in MPICH for clusters connected by switched networks. For each collective operation, we use multiple algorithms depending on the message size, with the goal of minimizing latency for short messages and minimizing bandwidth use for long messages. Although we have implemented new algorithms for all MPI Message Passing Interface collective operations, because of limited space we describe only the algorithms for allgather, broadcast, all-to-all, reduce-scatter, reduce, and allreduce. Performance results on a Myrinet-connected Linux cluster and an IBM SP indicate that, in all cases, the new algorithms significantly outperform the old algorithms used in MPICH on the Myrinet cluster, and, in many cases, they outperform the algorithms used in IBM's MPI on the SP. We also explore in further detail the optimization of two of the most commonly used collective operations, allreduce and reduce, particularly for long messages and nonpower-of-two numbers of processes. The optimized algorithms for these operations perform several times better than the native algorithms on a Myrinet cluster, IBM SP, and Cray T3E. Our results indicate that to achieve the best performance for a collective communication operation, one needs to use a number of different algorithms and select the right algorithm for a particular message size and number of processes.},
  numpages     = 18,
  keywords     = {reduction, MPI, message passing, Collective communication}
}
@article{platoxl,
  title        = {{PLATO-XL:} Exploring the Large-scale Pre-training of Dialogue Generation},
  author       = {Siqi Bao and Huang He and Fan Wang and Hua Wu and Haifeng Wang and Wenquan Wu and Zhihua Wu and Zhen Guo and Hua Lu and Xinxian Huang and Xin Tian and Xinchao Xu and Yingzhan Lin and Zhengyu Niu},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2109.09519},
  url          = {https://arxiv.org/abs/2109.09519},
  eprinttype   = {arXiv},
  eprint       = {2109.09519},
  timestamp    = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-09519.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{t5,
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year         = 2020,
  journal      = {Journal of Machine Learning Research},
  volume       = 21,
  number       = 140,
  pages        = {1--67},
  url          = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{adam2015atlas,
  title        = {ATLAS@ Home: harnessing volunteer computing for HEP},
  author       = {Adam-Bourdarios, C and Cameron, D and Filip{\v{c}}i{\v{c}}, A and Lancon, E and Wu, Wenjing and others},
  year         = 2015,
  booktitle    = {Journal of Physics: Conference Series},
  volume       = 664,
  number       = 2,
  pages        = {022009},
  organization = {IOP Publishing}
}
@article{li2017case,
  title        = {A Case Study of IPv6 Network Performance: Packet Delay, Loss, and Reordering},
  author       = {Li, Fuliang and Wang, Xingwei and Pan, Tian and Yang, Jiahai},
  year         = 2017,
  journal      = {Mathematical Problems in Engineering},
  publisher    = {Hindawi},
  volume       = 2017
}
@article{Sun2019OptimizingNP,
  title        = {Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author       = {Peng Sun and Wansen Feng and Ruobing Han and Shengen Yan and Yonggang Wen},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1902.06855}
}
@inproceedings{recht2011hogwild,
  title        = {Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author       = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  year         = 2011,
  booktitle    = {Advances in neural information processing systems},
  pages        = {693--701}
}
@article{zhang2015staleness,
  title        = {Staleness-aware async-sgd for distributed deep learning},
  author       = {Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
  year         = 2015,
  journal      = {arXiv preprint arXiv:1511.05950}
}
@misc{lc0,
  title        = {Leela Chess Zero},
  author       = {{Pascutto, Gian-Carlo and Linscott, Gary}},
  year         = 2019,
  url          = {http://lczero.org/},
  version      = {0.21.0}
}
@inproceedings{vc_evolve,
  title        = {Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters},
  author       = {T. {Desell}},
  year         = 2017,
  booktitle    = {2017 IEEE 13th International Conference on e-Science (e-Science)},
  volume       = {},
  number       = {},
  pages        = {19--28}
}
@article{moe_first,
  title        = {Adaptive Mixtures of Local Experts},
  author       = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year         = 1991,
  month        = mar,
  journal      = {Neural Computation},
  publisher    = {MIT Press},
  address      = {Cambridge, MA, USA},
  volume       = 3,
  number       = 1,
  pages        = {79–87},
  doi          = {10.1162/neco.1991.3.1.79},
  issn         = {0899-7667},
  url          = {https://doi.org/10.1162/neco.1991.3.1.79},
  issue_date   = {March 1991},
  numpages     = 9
}
@article{eigen2013learning,
  title        = {Learning factored representations in a deep mixture of experts},
  author       = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  year         = 2013,
  journal      = {arXiv preprint arXiv:1312.4314}
}
@article{jordan1994hierarchical,
  title        = {Hierarchical mixtures of experts and the EM algorithm},
  author       = {Jordan, Michael I and Jacobs, Robert A},
  year         = 1994,
  journal      = {Neural computation},
  publisher    = {MIT Press},
  volume       = 6,
  number       = 2,
  pages        = {181--214}
}
@inproceedings{yao2009hierarchical,
  title        = {Hierarchical mixture of classification experts uncovers interactions between brain regions},
  author       = {Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-Fei, Li},
  year         = 2009,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {2178--2186}
}
@inproceedings{rasmussen2002infinite,
  title        = {Infinite mixtures of Gaussian process experts},
  author       = {Rasmussen, Carl E and Ghahramani, Zoubin},
  year         = 2002,
  booktitle    = {Advances in neural information processing systems},
  pages        = {881--888}
}
@inproceedings{moe_lifelong,
  title        = {Expert Gate: Lifelong Learning with a Network of Experts},
  author       = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  year         = 2017,
  month        = {07},
  pages        = {7120--7129},
  doi          = {10.1109/CVPR.2017.753}
}
@inproceedings{moe_svm,
  title        = {A parallel mixture of SVMs for very large scale problems},
  author       = {Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  year         = 2002,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {633--640}
}
@article{moe_dirichlet,
  title        = {Nonlinear models using Dirichlet process mixtures},
  author       = {Shahbaba, Babak and Neal, Radford},
  year         = 2009,
  journal      = {Journal of Machine Learning Research},
  volume       = 10,
  number       = {Aug},
  pages        = {1829--1850}
}
@article{shazeer2017outrageously,
  title        = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author       = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year         = 2017,
  journal      = {arXiv preprint arXiv:1701.06538}
}
@incollection{pkm,
  title        = {Large Memory Layers with Product Keys},
  author       = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\' Aurelio and Denoyer, Ludovic and Jegou, Herve},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems 32},
  publisher    = {Curran Associates, Inc.},
  pages        = {8546--8557},
  url          = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf},
  editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{can,
  title        = {A scalable content-addressable network},
  author       = {Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
  year         = 2001,
  booktitle    = {Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
  pages        = {161--172}
}
@article{chord,
  title        = {Looking up data in P2P systems},
  author       = {Balakrishnan, Hari and Kaashoek, M Frans and Karger, David and Morris, Robert and Stoica, Ion},
  year         = 2003,
  journal      = {Communications of the ACM},
  publisher    = {ACM New York, NY, USA},
  volume       = 46,
  number       = 2,
  pages        = {43--48}
}
@inproceedings{pastry,
  title        = {Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems},
  author       = {Rowstron, Antony and Druschel, Peter},
  year         = 2001,
  booktitle    = {IFIP/ACM International Conference on Distributed Systems Platforms and Open Distributed Processing},
  pages        = {329--350},
  organization = {Springer}
}
@article{tapestry,
  title        = {Tapestry: A Resilient Global-Scale Overlay for Service Deployment},
  author       = {Zhao, Ben and Huang, Ling and Stribling, Jeremy and Rhea, Sean and Joseph, Anthony and Kubiatowicz, John},
  year         = 2003,
  month        = {07},
  journal      = {IEEE Journal on Selected Areas in Communications},
  volume       = 22,
  pages        = {},
  doi          = {10.1109/JSAC.2003.818784}
}
@techreport{tewari1998beyond,
  title        = {Beyond hierarchies: Design considerations for distributed caching on the internet},
  author       = {Tewari, Renu and Dahlin, Michael and Vin, Harrick and Kay, John},
  institution  = {Citeseer}
}
@misc{i2p,
  title        = {Invisible Internet Project (I2P) Project Overview},
  author       = {jrandom (Pseudonym)},
  year         = 2003,
  month        = {August},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://geti2p.net/_static/pdf/i2p_philosophy.pdf}}
}
@inproceedings{kademlia,
  title        = {Kademlia: A peer-to-peer information system based on the xor metric},
  author       = {Maymounkov, Petar and Mazieres, David},
  year         = 2002,
  booktitle    = {International Workshop on Peer-to-Peer Systems},
  pages        = {53--65},
  organization = {Springer}
}
@inproceedings{kaashoek2003koorde,
  title        = {Koorde: A simple degree-optimal distributed hash table},
  author       = {Kaashoek, M Frans and Karger, David R},
  year         = 2003,
  booktitle    = {International Workshop on Peer-to-Peer Systems},
  pages        = {98--107},
  organization = {Springer}
}
@inproceedings{mcmahan2017communication,
  title        = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author       = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  year         = 2017,
  booktitle    = {Artificial Intelligence and Statistics},
  pages        = {1273--1282}
}
@inproceedings{bonawitz2017practical,
  title        = {Practical secure aggregation for privacy-preserving machine learning},
  author       = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  year         = 2017,
  booktitle    = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages        = {1175--1191}
}
@article{srivastava2014dropout,
  title        = {Dropout: a simple way to prevent neural networks from overfitting},
  author       = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year         = 2014,
  journal      = {The journal of machine learning research},
  publisher    = {JMLR. org},
  volume       = 15,
  number       = 1,
  pages        = {1929--1958}
}
@article{stale_gradients_can_win,
  title        = {Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD},
  author       = {Dutta, Sanghamitra and Joshi, Gauri and Ghosh, Soumyadip and Dube, Parijat and Nagpurkar, Priya},
  year         = 2018,
  month        = {03},
  pages        = {}
}
@article{gradient_checkpointing_dl,
  title        = {Training deep nets with sublinear memory cost},
  author       = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  year         = 2016,
  journal      = {arXiv preprint arXiv:1604.06174}
}
@article{gradient_checkpointing_autograd,
  title        = {Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author       = {Griewank, Andreas and Walther, Andrea},
  year         = 2000,
  journal      = {ACM Transactions on Mathematical Software (TOMS)},
  publisher    = {ACM New York, NY, USA},
  volume       = 26,
  number       = 1,
  pages        = {19--45}
}
@misc{kaplan2020scaling,
  title        = {Scaling Laws for Neural Language Models},
  author       = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year         = 2020,
  eprint       = {2001.08361},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@misc{scaling2,
      title={Scaling Laws for Autoregressive Generative Modeling}, 
      author={Tom Henighan and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and Aditya Ramesh and Nick Ryder and Daniel M. Ziegler and John Schulman and Dario Amodei and Sam McCandlish},
      year={2020},
      eprint={2010.14701},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{dalle,
  title        = {Zero-shot text-to-image generation},
  author       = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year         = 2021,
  journal      = {arXiv preprint arXiv:2102.12092}
}
@article{scaling_vit,
  title        = {Scaling Vision Transformers},
  author       = {Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2106.04560},
  url          = {https://arxiv.org/abs/2106.04560},
  eprinttype   = {arXiv},
  eprint       = {2106.04560},
  timestamp    = {Fri, 11 Jun 2021 11:04:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-04560.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{coatnet,
  title        = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},
  author       = {Zihang Dai and Hanxiao Liu and Quoc V. Le and Mingxing Tan},
  year         = 2021,
  journal      = {ArXiv},
  volume       = {abs/2106.04803}
}
@article{vit_moe,
  title        = {Scaling Vision with Sparse Mixture of Experts},
  author       = {Carlos Riquelme and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and Andr{\'{e}} Susano Pinto and Daniel Keysers and Neil Houlsby},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2106.05974},
  url          = {https://arxiv.org/abs/2106.05974},
  eprinttype   = {arXiv},
  eprint       = {2106.05974},
  timestamp    = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-05974.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{guided_diffusion,
  title        = {Diffusion Models Beat GANs on Image Synthesis},
  author       = {Prafulla Dhariwal and Alex Nichol},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2105.05233},
  url          = {https://arxiv.org/abs/2105.05233},
  eprinttype   = {arXiv},
  eprint       = {2105.05233},
  timestamp    = {Fri, 14 May 2021 12:13:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2105-05233.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{infiniband,
  title        = {Introduction to infiniband for end users},
  author       = {Grun, Paul},
  year         = 2010,
  journal      = {White paper, InfiniBand Trade Association},
  volume       = 55
}
@inproceedings{zellers2019defending,
  title        = {Defending against neural fake news},
  author       = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {9051--9062}
}
@incollection{NIPS2019_8736,
  title        = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
  author       = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems 32},
  publisher    = {Curran Associates, Inc.},
  pages        = {4901--4910},
  url          = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf},
  editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{sybil_attacks_dht,
  title        = {Real-world sybil attacks in BitTorrent mainline DHT},
  author       = {Wang, Liang and Kangasharju, Jussi},
  year         = 2012,
  booktitle    = {2012 IEEE Global Communications Conference (GLOBECOM)},
  pages        = {826--832},
  organization = {IEEE}
}
@article{sybil_nodes,
  title        = {Sybil nodes as a mitigation strategy against sybil attack},
  author       = {Trifa, Zied and Khemakhem, Maher},
  year         = 2014,
  journal      = {Procedia Computer Science},
  publisher    = {Elsevier},
  volume       = 32,
  pages        = {1135--1140}
}
@inproceedings{dos_resistance,
  title        = {A denial-of-service resistant DHT},
  author       = {Awerbuch, Baruch and Scheideler, Christian},
  year         = 2007,
  booktitle    = {International Symposium on Distributed Computing},
  pages        = {33--47},
  organization = {Springer}
}
@article{urdaneta2011survey,
  title        = {A survey of DHT security techniques},
  author       = {Urdaneta, Guido and Pierre, Guillaume and Steen, Maarten Van},
  year         = 2011,
  journal      = {ACM Computing Surveys (CSUR)},
  publisher    = {ACM New York, NY, USA},
  volume       = 43,
  number       = 2,
  pages        = {1--49}
}
@article{bagdasaryan2018backdoor,
  title        = {How to backdoor federated learning},
  author       = {Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1807.00459}
}
@article{bhagoji2018analyzing,
  title        = {Analyzing federated learning through an adversarial lens},
  author       = {Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1811.12470}
}
@article{olah2018building,
  title        = {The building blocks of interpretability},
  author       = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  year         = 2018,
  journal      = {Distill},
  volume       = 3,
  number       = 3,
  pages        = {e10}
}
@article{seq2seqvis,
  title        = {Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
  author       = {H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
  year         = 2019,
  month        = {Jan},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  volume       = 25,
  number       = 1,
  pages        = {353--363},
  doi          = {10.1109/TVCG.2018.2865044},
  issn         = {2160-9306},
  keywords     = {data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP}
}
@article{carter2019activation,
  title        = {Activation atlas},
  author       = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  year         = 2019,
  journal      = {Distill},
  volume       = 4,
  number       = 3,
  pages        = {e15}
}
@inproceedings{pipedream,
  title        = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
  author       = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
  year         = 2019,
  booktitle    = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  location     = {Huntsville, Ontario, Canada},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {SOSP ’19},
  pages        = {1–15},
  doi          = {10.1145/3341301.3359646},
  isbn         = 9781450368735,
  url          = {https://doi.org/10.1145/3341301.3359646},
  numpages     = 15
}
@article{pipemare,
  title        = {PipeMare: Asynchronous Pipeline Parallel DNN Training},
  author       = {Bowen Yang and Jian Zhang and Jonathan Li and Christopher R{\'e} and Christopher R. Aberger and Christopher De Sa},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1910.05124}
}
@article{valiant1990bridging,
  title        = {A bridging model for parallel computation},
  author       = {Valiant, Leslie G},
  year         = 1990,
  journal      = {Communications of the ACM},
  publisher    = {ACM New York, NY, USA},
  volume       = 33,
  number       = 8,
  pages        = {103--111}
}
@misc{goyal2017accurate,
  title        = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author       = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  year         = 2017,
  eprint       = {1706.02677},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@inproceedings{You2020Large,
  title        = {Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
  author       = {Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=Syx4wnEtvH}
}
@article{natural_compression,
  title        = {Natural Compression for Distributed Deep Learning},
  author       = {Samuel Horvath and Chen{-}Yu Ho and Ludovit Horvath and Atal Narayan Sahu and Marco Canini and Peter Richt{\'{a}}rik},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1905.10988},
  url          = {http://arxiv.org/abs/1905.10988},
  archiveprefix = {arXiv},
  eprint       = {1905.10988},
  timestamp    = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{sukhov2016generating,
  title        = {Generating a function for network delay},
  author       = {Sukhov, Andrei M and Astrakhantseva, MA and Pervitsky, AK and Boldyrev, SS and Bukatov, AA},
  year         = 2016,
  journal      = {Journal of High Speed Networks},
  publisher    = {IOS Press},
  volume       = 22,
  number       = 4,
  pages        = {321--333}
}
@inproceedings{jaderberg2017decoupled,
  title        = {Decoupled neural interfaces using synthetic gradients},
  author       = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  year         = 2017,
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {1627--1635},
  organization = {JMLR. org}
}
@misc{ma2019hsic,
  title        = {The HSIC Bottleneck: Deep Learning without Back-Propagation},
  author       = {Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
  year         = 2019,
  eprint       = {1908.01580},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@inproceedings{real2017large,
  title        = {Large-scale evolution of image classifiers},
  author       = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  year         = 2017,
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {2902--2911},
  organization = {JMLR. org}
}
@misc{hendrycks2016gaussian,
  title        = {Gaussian Error Linear Units (GELUs)},
  author       = {Dan Hendrycks and Kevin Gimpel},
  year         = 2016,
  eprint       = {1606.08415},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@article{mnist,
  title        = {Gradient-based learning applied to document recognition},
  author       = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year         = 1998,
  journal      = {Proceedings of the IEEE},
  publisher    = {Ieee},
  volume       = 86,
  number       = 11,
  pages        = {2278--2324}
}
@inproceedings{zero,
  title        = {ZeRO: Memory Optimization Towards Training A Trillion Parameter Models},
  author       = {Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
  year         = 2020,
  booktitle    = {SC}
}
@misc{tnlg,
  title        = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
  author       = {Corby Rosset},
  howpublished = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}
}
@misc{fugaku,
  title        = {Fugaku},
  author       = {Erich Strohmaier and Jack Dongarra and Horst Simon and Martin Meuer},
  year         = 2021,
  note         = {Estimated energy consumption 29,899.23 kW. Accessed: 2021-10-4},
  howpublished = {https://www.top500.org/system/179807/}
}
@misc{microsoft_supercomputer,
  title        = {Microsoft announces new supercomputer, lays out vision for future AI work},
  author       = {Jennifer Langston},
  year         = 2020,
  note         = {Accessed: 2021-10-1},
  howpublished = {https://blogs.microsoft.com/ai/openai-azure-supercomputer/}
}

@misc{openai-api,
  title        = {Build next-gen apps with
OpenAI’s powerful models.},
  author       = {OpenAI},
  note         = {Accessed: 2022-06-22},
  howpublished = {https://openai.com/api}
}

@misc{jurrasic,
  title        = {Jurassic-1 Language Models},
  author       = {AI21},
  note         = {Accessed: 2022-06-22},
  howpublished = {"\url{https://studio.ai21.com/docs/jurassic1-language-models}"}
}

@misc{forefront,
  title        = {Powerful language models a click away},
  author       = {Forefront},
  note         = {Accessed: 2022-06-22},
  howpublished = {"\url{https://www.forefront.ai/}"}
}



@inbook{summit,
  title        = {Scaling the Summit: Deploying the World’s Fastest Supercomputer},
  author       = {Larrea, Verónica and Joubert, Wayne and Brim, Michael and Budiardja, Reuben and Maxwell, Don and Ezell, Matt and Zimmer, Christopher and Boehm, Swen and Elwasif, Wael and Oral, Sarp and Fuson, Chris and Pelfrey, Daniel and Hernandez, Oscar and Leverman, Dustin and Hanley, Jesse and Berrill, Mark and Tharrington, Arnold},
  year         = 2019,
  month        = 12,
  pages        = {330--351},
  doi          = {10.1007/978-3-030-34356-9_26},
  isbn         = {978-3-030-34355-2}
}
@misc{gpt3cost,
  author       = {Elliot Turner},
  note         = {Estimate of GPT-3 training cost based on public cloud GPU/TPU cost models, from Elliot Turner's personal page (accessed on May 29, 2020)}
}
@misc{lambdabenchmarks,
  title        = {Deep Learning GPU benchmarks, Lambda Labs website, 2018/10/08},
  author       = {Stephen Balaban , Chuan Li}
}
@inproceedings{paszke2019pytorch,
  title        = {PyTorch: An imperative style, high-performance deep learning library},
  author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {8024--8035}
}
@inproceedings{desell2017,
  title        = {Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters},
  author       = {T. {Desell}},
  year         = 2017,
  booktitle    = {2017 IEEE 13th International Conference on e-Science (e-Science)},
  volume       = {},
  number       = {},
  pages        = {19--28}
}
@inproceedings{deepgradientcompression,
  title        = {{Deep Gradient Compression: Reducing the communication bandwidth for distributed training}},
  author       = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  year         = 2018,
  booktitle    = {The International Conference on Learning Representations}
}
@inproceedings{li2020acceleration,
  title        = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization},
  author       = {Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richtarik, Peter},
  year         = 2020,
  month        = {13--18 Jul},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 119,
  pages        = {5895--5904},
  url          = {http://proceedings.mlr.press/v119/li20g.html},
  editor       = {Hal Daumé III and Aarti Singh},
  pdf          = {http://proceedings.mlr.press/v119/li20g/li20g.pdf}
}
@inproceedings{koloskova2020decentralized,
  title        = {Decentralized Deep Learning with Arbitrary Communication Compression},
  author       = {Anastasia Koloskova* and Tao Lin* and Sebastian U Stich and Martin Jaggi},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=SkgGCkrKvH}
}
@article{wagma,
  title        = {Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging},
  author       = {Li, Shigang and Ben-Nun, Tal and Nadiradze, Giorgi and Digirolamo, Salvatore and Dryden, Nikoli and Alistarh, Dan and Hoefler, Torsten},
  year         = 2020,
  journal      = {IEEE Transactions on Parallel and Distributed Systems},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
  pages        = {1–1},
  doi          = {10.1109/tpds.2020.3040606},
  issn         = {2161-9883},
  url          = {http://dx.doi.org/10.1109/TPDS.2020.3040606}
}
@misc{moshpit,
  title        = {Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices},
  author       = {Max Ryabinin and Eduard Gorbunov and Vsevolod Plokhotnyuk and Gennady Pekhimenko},
  year         = 2021,
  eprint       = {2103.03239},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@inproceedings{ps,
  title        = {Scaling Distributed Machine Learning with the Parameter Server},
  author       = {Mu Li and D. Andersen and J. Park and Alex Smola and Amr Ahmed and V. Josifovski and J. Long and E. Shekita and Bor-Yiing Su},
  year         = 2014,
  booktitle    = {BigDataScience '14}
}
@article{l2l,
  title        = {Training large neural networks with constant memory using a new execution algorithm},
  author       = {Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2002.05645}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@misc{hu2021lora,
    title={LoRA: Low-Rank Adaptation of Large Language Models},
    author={Hu, Edward and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},
    year={2021},
    eprint={2106.09685},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{ptune-liu,
    title={GPT Understands, Too},
    author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
    journal={arXiv:2103.10385},
    year={2021}
    }

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}


@inproceedings{ptune-lester,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}

@article{ptune-v2,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}



@misc{zerooffload,
  title        = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  author       = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
  year         = 2021,
  eprint       = {2101.06840},
  archiveprefix = {arXiv},
  primaryclass = {cs.DC}
}
@inproceedings{byteps,
  title        = {A Unified Architecture for Accelerating Distributed {DNN} Training in Heterogeneous GPU/CPU Clusters},
  author       = {Yimin Jiang and Yibo Zhu and Chang Lan and Bairen Yi and Yong Cui and Chuanxiong Guo},
  year         = 2020,
  month        = nov,
  booktitle    = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
  publisher    = {{USENIX} Association},
  pages        = {463--479},
  isbn         = {978-1-939133-19-9},
  url          = {https://www.usenix.org/conference/osdi20/presentation/jiang}
}
@misc{horvath2019natural,
  title        = {Natural Compression for Distributed Deep Learning},
  author       = {Samuel Horvath and Chen-Yu Ho and Ludovit Horvath and Atal Narayan Sahu and Marco Canini and Peter Richtarik},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1905.10988},
  eprint       = {1905.10988},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@inproceedings{sgp,
  title        = {Stochastic Gradient Push for Distributed Deep Learning},
  author       = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  year         = 2019,
  month        = {09--15 Jun},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 97,
  pages        = {344--353},
  url          = {http://proceedings.mlr.press/v97/assran19a.html},
  editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pdf          = {http://proceedings.mlr.press/v97/assran19a/assran19a.pdf}
}
@inproceedings{slowmo,
  title        = {SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum},
  author       = {Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=SkxJ8REYPH}
}
@misc{mikami2019massively,
  title        = {Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash},
  author       = {Hiroaki Mikami and Hisahiro Suganuma and Pongsakorn U-chupala and Yoshiki Tanaka and Yuichi Kageyama},
  year         = 2019,
  eprint       = {1811.05233},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@misc{pytorch_elastic,
  title        = {{PyTorch Elastic}},
  author       = {TorchElastic},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://pytorch.org/elastic}}
}
@misc{elastic_horovod,
  title        = {{Elastic Horovod}},
  author       = {ElasticHorovod},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{ https://horovod.readthedocs.io/en/stable/elastic_include.html}}
}
@misc{verizon_latency,
  title        = {Monthly IP Latency Data},
  author       = {Verizon},
  year         = 2021,
  note         = {Accessed: 2021-10-05},
  howublished  = {https://www.verizon.com/business/terms/latency/}
}
@inproceedings{parameter_server_first,
  title        = {Scaling Distributed Machine Learning with the Parameter Server},
  author       = {Li, Mu},
  year         = 2014,
  booktitle    = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
  location     = {Beijing, China},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {BigDataScience '14},
  doi          = {10.1145/2640087.2644155},
  isbn         = 9781450328913,
  url          = {https://doi.org/10.1145/2640087.2644155},
  abstract     = {Big data may contain big values, but also brings lots of challenges to the computing theory, architecture, framework, knowledge discovery algorithms, and domain specific tools and applications. Beyond the 4-V or 5-V characters of big datasets, the data processing shows the features like inexact, incremental, and inductive manner. This brings new research opportunities to research community across theory, systems, algorithms, and applications. Is there some new "theory" for the big data? How to handle the data computing algorithms in an operatable manner? This report shares some view on new challenges identified, and covers some of the application scenarios such as micro-blog data analysis and data processing in building next generation search engines.},
  articleno    = 3,
  numpages     = 1
}
@inproceedings{Biggadike05natblaster:establishing,
  title        = {NATBLASTER: Establishing TCP connections between hosts behind NATs},
  author       = {Andrew Biggadike and Daniel Ferullo and Geoffrey Wilson and Adrian Perrig},
  year         = 2005,
  booktitle    = {IN PROCEEDINGS OF ACM SIGCOMM ASIA WORKSHOP},
  publisher    = {}
}
@article{DBLP:journals/corr/abs-cs-0603074,
  title        = {Peer-to-Peer Communication Across Network Address Translators},
  author       = {Bryan Ford and Pyda Srisuresh and Dan Kegel},
  year         = 2006,
  journal      = {CoRR},
  volume       = {abs/cs/0603074},
  url          = {http://arxiv.org/abs/cs/0603074},
  archiveprefix = {arXiv},
  eprint       = {cs/0603074},
  timestamp    = {Mon, 13 Aug 2018 16:48:07 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-cs-0603074.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Oscar,
  title        = {A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages},
  author       = {Ortiz Su{\'a}rez, Pedro Javier  and Romary, Laurent  and Sagot, Beno{\^\i}t},
  year         = 2020,
  month        = jul,
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {1703--1714},
  url          = {https://www.aclweb.org/anthology/2020.acl-main.156},
  abstract     = {We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.}
}
@article{kudo2018subword,
  title        = {Subword regularization: Improving neural network translation models with multiple subword candidates},
  author       = {Kudo, Taku},
  year         = 2018,
  journal      = {arXiv preprint arXiv:1804.10959}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{feyzmahdavian2016asynchronous,
  title        = {An asynchronous mini-batch algorithm for regularized stochastic optimization},
  author       = {Feyzmahdavian, Hamid Reza and Aytekin, Arda and Johansson, Mikael},
  year         = 2016,
  journal      = {IEEE Transactions on Automatic Control},
  publisher    = {IEEE},
  volume       = 61,
  number       = 12,
  pages        = {3740--3754}
}
@inproceedings{arjevani2020tight,
  title        = {A tight convergence analysis for stochastic gradient descent with delayed updates},
  author       = {Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan},
  year         = 2020,
  booktitle    = {Algorithmic Learning Theory},
  pages        = {111--132},
  organization = {PMLR}
}
@inproceedings{MLSYS2019_d09bf415,
  title        = {Priority-based Parameter Propagation for Distributed DNN Training},
  author       = {Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
  year         = 2019,
  booktitle    = {Proceedings of Machine Learning and Systems},
  volume       = 1,
  pages        = {132--145},
  url          = {https://proceedings.mlsys.org/paper/2019/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf},
  editor       = {A. Talwalkar and V. Smith and M. Zaharia}
}
@inproceedings{agarwal2011distributed,
  title        = {Distributed delayed stochastic optimization},
  author       = {Agarwal, Alekh and Duchi, John C},
  year         = 2011,
  booktitle    = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
  pages        = {873--881}
}
@inproceedings{mishchenko2018delay,
  title        = {A delay-tolerant proximal-gradient algorithm for distributed learning},
  author       = {Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Amini, Massih-Reza},
  year         = 2018,
  booktitle    = {International Conference on Machine Learning},
  pages        = {3587--3595},
  organization = {PMLR}
}
@article{peng2016arock,
  title        = {Arock: an algorithmic framework for asynchronous parallel coordinate updates},
  author       = {Peng, Zhimin and Xu, Yangyang and Yan, Ming and Yin, Wotao},
  year         = 2016,
  journal      = {SIAM Journal on Scientific Computing},
  publisher    = {SIAM},
  volume       = 38,
  number       = 5,
  pages        = {A2851--A2879}
}
@inproceedings{leblond2017asaga,
  title        = {ASAGA: asynchronous parallel SAGA},
  author       = {Leblond, R{\'e}mi and Pedregosa, Fabian and Lacoste-Julien, Simon},
  year         = 2017,
  booktitle    = {Artificial Intelligence and Statistics},
  pages        = {46--54},
  organization = {PMLR}
}
@inproceedings{zhao2016fast,
  title        = {Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee},
  author       = {Zhao, Shen-Yi and Li, Wu-Jun},
  year         = 2016,
  booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = 30,
  number       = 1
}
@article{assran2020advances,
  title        = {Advances in Asynchronous Parallel and Distributed Optimization},
  author       = {Assran, Mahmoud and Aytekin, Arda and Feyzmahdavian, Hamid Reza and Johansson, Mikael and Rabbat, Michael G},
  year         = 2020,
  journal      = {Proceedings of the IEEE},
  publisher    = {IEEE},
  volume       = 108,
  number       = 11,
  pages        = {2013--2031}
}
@inproceedings{basu2019qsparse,
  title        = {Qsparse-local-{SGD}: Distributed {SGD} with Quantization, Sparsification and Local Computations},
  author       = {Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {14668--14679}
}
@article{yuan2020federated_comp,
  title        = {Federated Composite Optimization},
  author       = {Yuan, Honglin and Zaheer, Manzil and Reddi, Sashank},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2011.08474}
}
@article{yuan2020federated,
  title        = {Federated Accelerated Stochastic Gradient Descent},
  author       = {Yuan, Honglin and Ma, Tengyu},
  year         = 2020,
  journal      = {Advances in Neural Information Processing Systems},
  volume       = 33
}
@article{woodworth2020minibatch,
  title        = {Minibatch vs local sgd for heterogeneous distributed learning},
  author       = {Woodworth, Blake and Patel, Kumar Kshitij and Srebro, Nathan},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2006.04735}
}
@article{LinSPJ2018local,
  title        = {Don't Use Large Mini-Batches, Use Local {SGD}},
  author       = {Tao Lin and Sebastian Urban Stich and Kumar Kshitij Patel and Martin Jaggi},
  year         = 2020,
  journal      = {ICLR},
  pages        = {arXiv:1808.07217},
  url          = {https://arxiv.org/abs/1808.07217}
}
@article{Stich18local,
  title        = {Local {SGD} Converges Fast and Communicates Little},
  author       = {Sebastian Urban Stich},
  year         = 2019,
  journal      = {International Conference on Learning Representations (ICLR)},
  pages        = {arXiv:1805.09767},
  url          = {https://arxiv.org/abs/1805.09767}
}
@article{kairouz2019advances,
  title        = {Advances and open problems in federated learning},
  author       = {Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1912.04977}
}
@article{konevcny2016federated,
  title        = {Federated learning: Strategies for improving communication efficiency},
  author       = {Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  year         = 2016,
  journal      = {arXiv preprint arXiv:1610.05492}
}
@article{kovalev2020linearly,
  title        = {A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!},
  author       = {Kovalev, Dmitry and Koloskova, Anastasia and Jaggi, Martin and Richtarik, Peter and Stich, Sebastian U},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2011.01697}
}
@article{reisizadeh2019exact,
  title        = {An exact quantized decentralized gradient descent algorithm},
  author       = {Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Pedarsani, Ramtin},
  year         = 2019,
  journal      = {IEEE Transactions on Signal Processing},
  publisher    = {IEEE},
  volume       = 67,
  number       = 19,
  pages        = {4934--4947}
}
@article{qian2020error,
  title        = {Error Compensated Distributed SGD Can Be Accelerated},
  author       = {Qian, Xun and Richt{\'a}rik, Peter and Zhang, Tong},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2010.00091}
}
@inproceedings{karimireddy2019error,
  title        = {Error feedback fixes signsgd and other gradient compression schemes},
  author       = {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning},
  pages        = {3252--3261},
  organization = {PMLR}
}
@inproceedings{stich2018sparsified,
  title        = {Sparsified SGD with memory},
  author       = {Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  year         = 2018,
  booktitle    = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages        = {4452--4463}
}
@article{das2020improved,
  title        = {Improved Convergence Rates for Non-Convex Federated Learning with Compression},
  author       = {Das, Rudrajit and Hashemi, Abolfazl and Sanghavi, Sujay and Dhillon, Inderjit S},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2012.04061}
}
@article{haddadpour2020federated,
  title        = {Federated learning with compression: Unified analysis and sharp guarantees},
  author       = {Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2007.01154}
}
@article{li2020unified,
  title        = {A unified analysis of stochastic gradient methods for nonconvex federated optimization},
  author       = {Li, Zhize and Richt{\'a}rik, Peter},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2006.07013}
}
@article{philippenko2020artemis,
  title        = {Artemis: tight convergence guarantees for bidirectional compression in federated learning},
  author       = {Philippenko, Constantin and Dieuleveut, Aymeric},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2006.14591}
}
@article{gorbunov2020linearly,
  title        = {Linearly Converging Error Compensated SGD},
  author       = {Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  year         = 2020,
  journal      = {Advances in Neural Information Processing Systems},
  volume       = 33
}
@article{horvath2019stochastic,
  title        = {Stochastic distributed learning with gradient quantization and variance reduction},
  author       = {Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt{\'a}rik, Peter},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1904.05115}
}
@article{mishchenko2019distributed,
  title        = {Distributed learning with compressed gradient differences},
  author       = {Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1901.09269}
}
@inproceedings{wen2017terngrad,
  title        = {TernGrad: ternary gradients to reduce communication in distributed deep learning},
  author       = {Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  year         = 2017,
  booktitle    = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages        = {1508--1518}
}
@article{beznosikov2020biased,
  title        = {On biased compression for distributed learning},
  author       = {Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2002.12410}
}
@inproceedings{alistarh2017qsgd,
  title        = {QSGD: communication-efficient SGD via gradient quantization and encoding},
  author       = {Alistarh, Dan and Grubic, Demjan and Li, Jerry Z and Tomioka, Ryota and Vojnovic, Milan},
  year         = 2017,
  booktitle    = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages        = {1707--1718}
}
@inproceedings{suresh2017distributed,
  title        = {Distributed mean estimation with limited communication},
  author       = {Suresh, Ananda Theertha and Felix, X Yu and Kumar, Sanjiv and McMahan, H Brendan},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning},
  pages        = {3329--3337},
  organization = {PMLR}
}
@inproceedings{seide20141,
  title        = {1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author       = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  year         = 2014,
  booktitle    = {Fifteenth Annual Conference of the International Speech Communication Association}
}
@article{li2019communication,
  title        = {Communication efficient decentralized training with multiple local updates},
  author       = {Li, Xiang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1910.09126},
  volume       = 5
}
@inproceedings{karimireddy2020scaffold,
  title        = {SCAFFOLD: Stochastic controlled averaging for federated learning},
  author       = {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  year         = 2020,
  booktitle    = {International Conference on Machine Learning},
  pages        = {5132--5143},
  organization = {PMLR}
}
@inproceedings{koloskova2020unified,
  title        = {A unified theory of decentralized SGD with changing topology and local updates},
  author       = {Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  year         = 2020,
  booktitle    = {International Conference on Machine Learning},
  pages        = {5381--5393},
  organization = {PMLR}
}
@inproceedings{khaled2020tighter,
  title        = {Tighter theory for local SGD on identical and heterogeneous data},
  author       = {Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  year         = 2020,
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {4519--4529},
  organization = {PMLR}
}
@inproceedings{woodworth2020local,
  title        = {Is local SGD better than minibatch SGD?},
  author       = {Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  year         = 2020,
  booktitle    = {International Conference on Machine Learning},
  pages        = {10334--10343},
  organization = {PMLR}
}
@article{gorbunov2020local,
  title        = {Local sgd: Unified theory and new efficient methods},
  author       = {Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2011.02828}
}
@inproceedings{gower2019sgd,
  title        = {SGD: General analysis and improved rates},
  author       = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning},
  pages        = {5200--5209},
  organization = {PMLR}
}
@article{ghadimi2013stochastic,
  title        = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author       = {Ghadimi, Saeed and Lan, Guanghui},
  year         = 2013,
  journal      = {SIAM Journal on Optimization},
  publisher    = {SIAM},
  volume       = 23,
  number       = 4,
  pages        = {2341--2368}
}
@article{nemirovski2009robust,
  title        = {Robust stochastic approximation approach to stochastic programming},
  author       = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  year         = 2009,
  journal      = {SIAM Journal on optimization},
  publisher    = {SIAM},
  volume       = 19,
  number       = 4,
  pages        = {1574--1609}
}
@inproceedings{lian2017can,
  title        = {Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author       = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {5330--5340}
}
@inproceedings{mryab,
  title        = {Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts},
  author       = {Max Ryabinin and Anton Gusev},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems},
  eprint       = {2002.04013},
  archiveprefix = {arXiv},
  primaryclass = {cs.DC}
}
@inproceedings{scaman2017optimal,
  title        = {Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks},
  author       = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  year         = 2017,
  booktitle    = {International Conference on Machine Learning},
  pages        = {3027--3036}
}
@inproceedings{scaman2018optimal,
  title        = {Optimal algorithms for non-smooth distributed optimization in networks},
  author       = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems},
  pages        = {2740--2749}
}
@inproceedings{assran2019stochastic,
  title        = {Stochastic gradient push for distributed deep learning},
  author       = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  year         = 2019,
  booktitle    = {International Conference on Machine Learning},
  pages        = {344--353},
  organization = {PMLR}
}
@article{xiao2004fast,
  title        = {Fast linear iterations for distributed averaging},
  author       = {Xiao, Lin and Boyd, Stephen},
  year         = 2004,
  journal      = {Systems \& Control Letters},
  publisher    = {Elsevier},
  volume       = 53,
  number       = 1,
  pages        = {65--78}
}
@article{boyd2006randomized,
  title        = {Randomized gossip algorithms},
  author       = {Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  year         = 2006,
  journal      = {IEEE transactions on information theory},
  publisher    = {IEEE},
  volume       = 52,
  number       = 6,
  pages        = {2508--2530}
}
@article{merris1994laplacian,
  title        = {Laplacian matrices of graphs: a survey},
  author       = {Merris, Russell},
  year         = 1994,
  journal      = {Linear algebra and its applications},
  publisher    = {Elsevier},
  volume       = 197,
  pages        = {143--176}
}
@article{uribe2020dual,
  title        = {A dual approach for optimal algorithms in distributed optimization over networks},
  author       = {Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  year         = 2020,
  journal      = {Optimization Methods and Software},
  publisher    = {Taylor \& Francis},
  pages        = {1--40}
}
@techreport{tsitsiklis1984problems,
  title        = {Problems in decentralized decision making and computation.},
  author       = {Tsitsiklis, John Nikolas},
  year         = 1984,
  institution  = {Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems}
}
@article{scaman2019optimal,
  title        = {Optimal convergence rates for convex distributed optimization in networks},
  author       = {Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin and Massouli{\'e}, Laurent},
  year         = 2019,
  journal      = {Journal of Machine Learning Research},
  volume       = 20,
  pages        = {1--31}
}
@article{xu2020distributed,
  title        = {Distributed Algorithms for Composite Optimization: Unified and Tight Convergence Analysis},
  author       = {Xu, Jinming and Tian, Ye and Sun, Ying and Scutari, Gesualdo},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2002.11534}
}
@article{kovalev2020optimal,
  title        = {Optimal and practical algorithms for smooth and strongly convex decentralized optimization},
  author       = {Kovalev, Dmitry and Salim, Adil and Richt{\'a}rik, Peter},
  year         = 2020,
  journal      = {Advances in Neural Information Processing Systems},
  volume       = 33
}
@article{arjevani2015communication,
  title        = {Communication complexity of distributed convex learning and optimization},
  author       = {Arjevani, Yossi and Shamir, Ohad},
  year         = 2015,
  journal      = {Advances in neural information processing systems},
  volume       = 28,
  pages        = {1756--1764}
}
@article{fallah2019robust,
  title        = {Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks},
  author       = {Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asu and Simsekli, Umut and Zhu, Lingjiong},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1910.08701}
}
@article{nedic2014distributed,
  title        = {Distributed optimization over time-varying directed graphs},
  author       = {Nedi{\'c}, Angelia and Olshevsky, Alex},
  year         = 2014,
  journal      = {IEEE Transactions on Automatic Control},
  publisher    = {IEEE},
  volume       = 60,
  number       = 3,
  pages        = {601--615}
}
@article{nedic2016stochastic,
  title        = {Stochastic gradient-push for strongly convex functions on time-varying directed graphs},
  author       = {Nedi{\'c}, Angelia and Olshevsky, Alex},
  year         = 2016,
  journal      = {IEEE Transactions on Automatic Control},
  publisher    = {IEEE},
  volume       = 61,
  number       = 12,
  pages        = {3936--3947}
}
@article{nedic2018network,
  title        = {Network topology and communication-computation tradeoffs in decentralized optimization},
  author       = {Nedi{\'c}, Angelia and Olshevsky, Alex and Rabbat, Michael G},
  year         = 2018,
  journal      = {Proceedings of the IEEE},
  publisher    = {IEEE},
  volume       = 106,
  number       = 5,
  pages        = {953--976}
}
@article{rogozin2019projected,
  title        = {Projected gradient method for decentralized optimization over time-varying networks},
  author       = {Rogozin, Alexander and Gasnikov, Alexander},
  year         = 2019,
  journal      = {arXiv preprint arXiv:1911.08527}
}
@inproceedings{ram2009asynchronous,
  title        = {Asynchronous gossip algorithms for stochastic optimization},
  author       = {Ram, S Sundhar and Nedi{\'c}, A and Veeravalli, Venugopal V},
  year         = 2009,
  booktitle    = {Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference},
  pages        = {3581--3586},
  organization = {IEEE}
}
@article{yan2012distributed,
  title        = {Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties},
  author       = {Yan, Feng and Sundaram, Shreyas and Vishwanathan, SVN and Qi, Yuan},
  year         = 2012,
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  publisher    = {IEEE},
  volume       = 25,
  number       = 11,
  pages        = {2483--2493}
}
@article{yuan2016convergence,
  title        = {On the convergence of decentralized gradient descent},
  author       = {Yuan, Kun and Ling, Qing and Yin, Wotao},
  year         = 2016,
  journal      = {SIAM Journal on Optimization},
  publisher    = {SIAM},
  volume       = 26,
  number       = 3,
  pages        = {1835--1854}
}
@inproceedings{squad,
  title        = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author       = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  year         = 2016,
  booktitle    = {EMNLP}
}
@misc{aldous2002reversible,
  title        = {Reversible markov chains and random walks on graphs, 2002. Unfinished monograph, recompiled 2014},
  author       = {Aldous, David and Fill, James Allen},
  year         = 2002
}
@misc{minimax,
  title        = {SA305 – Linear Programming, Lesson 32. Maximin and Minimax Objectives},
  author       = {David Phillips},
  year         = 2015
}
@incollection{andersen,
  title        = {The Mosek Interior Point Optimizer for Linear Programming: An Implementation of the Homogeneous Algorithm},
  author       = {Erling D. Andersen and Knud D. Andersen},
  year         = 2000,
  booktitle    = {Applied Optimization},
  publisher    = {Springer {US}},
  pages        = {197--232},
  doi          = {10.1007/978-1-4757-3216-0_8},
  url          = {https://doi.org/10.1007%2F978-1-4757-3216-0_8}
}
@misc{Torrey_transferlearning,
  title        = {Transfer Learning},
  author       = {Lisa Torrey and Jude Shavlik},
  year         = {}
}
@article{Dettmers20158BitAF,
  title        = {8-Bit Approximations for Parallelism in Deep Learning},
  author       = {Tim Dettmers},
  year         = 2015,
  journal      = {ICLR}
}

@article{dettmers2022optimizers,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{dettmers2022llm,
  title        = {{LLM}.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author       = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  year         = 2022,
  journal      = {ArXiv},
  volume       = {abs/2208.07339}
}


@inproceedings{jft-300m,
  title        = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  author       = {Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta},
  year         = 2017,
  booktitle    = {ICCV},
  url          = {https://arxiv.org/abs/1707.02968}
}
@inproceedings{Kolesnikov2020BigT,
  title        = {Big Transfer (BiT): General Visual Representation Learning},
  author       = {Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and S. Gelly and N. Houlsby},
  year         = 2020,
  booktitle    = {ECCV}
}
@inproceedings{xlnet,
  title        = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author       = {Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
  year         = 2019,
  booktitle    = {NeurIPS}
}
@inproceedings{albert,
  title        = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author       = {Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  year         = 2020,
  booktitle    = {International Conference on Learning Representations}
}
@article{gpt3,
  title        = {Language Models are Few-Shot Learners},
  author       = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2005.14165}
}
@article{ringallreduce,
  title        = {Bandwidth optimal all-reduce algorithms for clusters of workstations},
  author       = {Patarasuk, Pitch and Yuan, Xin},
  year         = 2009,
  month        = {02},
  journal      = {Journal of Parallel and Distributed Computing},
  volume       = 69,
  pages        = {117--124},
  doi          = {10.1016/j.jpdc.2008.09.002}
}
@article{bandwidth_optimal_allreduce,
  title        = {Bandwidth Optimal All-Reduce Algorithms for Clusters of Workstations},
  author       = {Patarasuk, Pitch and Yuan, Xin},
  year         = 2009,
  month        = feb,
  journal      = {J. Parallel Distrib. Comput.},
  publisher    = {Academic Press, Inc.},
  address      = {USA},
  volume       = 69,
  number       = 2,
  pages        = {117–124},
  doi          = {10.1016/j.jpdc.2008.09.002},
  issn         = {0743-7315},
  url          = {https://doi.org/10.1016/j.jpdc.2008.09.002},
  issue_date   = {February, 2009},
  abstract     = {We consider an efficient realization of the all-reduce operation with large data sizes in cluster environments, under the assumption that the reduce operator is associative and commutative. We derive a tight lower bound of the amount of data that must be communicated in order to complete this operation and propose a ring-based algorithm that only requires tree connectivity to achieve bandwidth optimality. Unlike the widely used butterfly-like all-reduce algorithm that incurs network contention in SMP/multi-core clusters, the proposed algorithm can achieve contention-free communication in almost all contemporary clusters, including SMP/multi-core clusters and Ethernet switched clusters with multiple switches. We demonstrate that the proposed algorithm is more efficient than other algorithms on clusters with different nodal architectures and networking technologies when the data size is sufficiently large.},
  numpages     = 8,
  keywords     = {Tree topology, Collective communication, All-reduce, Cluster of workstations}
}
@article{torus_allreduce,
  title        = {Collective Algorithms for Multiported Torus Networks},
  author       = {Sack, Paul and Gropp, William},
  year         = 2015,
  month        = feb,
  journal      = {ACM Trans. Parallel Comput.},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = 1,
  number       = 2,
  doi          = {10.1145/2686882},
  issn         = {2329-4949},
  url          = {https://doi.org/10.1145/2686882},
  issue_date   = {January 2015},
  abstract     = {Modern supercomputers with torus networks allow each node to simultaneously pass messages on all of its links. However, most collective algorithms are designed to only use one link at a time. In this work, we present novel multiported algorithms for the scatter, gather, all-gather, and reduce-scatter operations. Our algorithms can be combined to create multiported reduce, all-reduce, and broadcast algorithms. Several of these algorithms involve a new technique where we relax the MPI message-ordering constraints to achieve high performance and restore the correctordering using an additional stage of redundant communication.According to our models, on an n-dimensional torus, our algorithms should allow for nearly a 2n-fold improvement in communication performance compared to known, single-ported torus algorithms. In practice, we have achieved nearly 6x better performance on a 32k-node 3-dimensional torus.},
  articleno    = 12,
  numpages     = 33,
  keywords     = {collective algorithms, Message-passing}
}
@misc{speedtest,
  title        = {Speedtest Global Index for Fixed Broadband},
  note         = {\url{https://www.speedtest.net/global-index} (accessed on 11.08.2020, bandwidth for top countries and general trend)}
}
@inproceedings{survey_distributed2,
  title        = {Performance Analysis and Comparison of Distributed Machine Learning Systems},
  author       = {Alqahtani, Salem and Demirbas, Murat},
  year         = 2019,
  month        = {07},
  pages        = {}
}
@article{dean12,
  title        = {Large scale distributed deep networks},
  author       = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  year         = 2012,
  journal      = {Advances in neural information processing systems},
  volume       = 25,
  pages        = {1223--1231}
}
@inproceedings{coates13,
  title        = {Deep learning with COTS HPC systems},
  author       = {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David and Catanzaro, Bryan and Andrew, Ng},
  year         = 2013,
  month        = {17--19 Jun},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
  publisher    = {PMLR},
  address      = {Atlanta, Georgia, USA},
  series       = {Proceedings of Machine Learning Research},
  volume       = 28,
  number       = 3,
  pages        = {1337--1345},
  url          = {https://proceedings.mlr.press/v28/coates13.html},
  editor       = {Dasgupta, Sanjoy and McAllester, David},
  pdf          = {http://proceedings.mlr.press/v28/coates13.pdf},
  abstract     = {Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features.  Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores.  In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI.  Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines.  As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.}
}
@inproceedings{sharded_ps_first,
  title        = {Large Scale Distributed Deep Networks},
  author       = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
  year         = 2012,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 25,
  pages        = {1223--1231},
  url          = {https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
  editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}
@inproceedings{pmlr-v97-koloskova19a,
  title        = {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication},
  author       = {Koloskova, Anastasia and Stich, Sebastian and Jaggi, Martin},
  year         = 2019,
  month        = {09--15 Jun},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 97,
  pages        = {3478--3487},
  url          = {http://proceedings.mlr.press/v97/koloskova19a.html},
  editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pdf          = {http://proceedings.mlr.press/v97/koloskova19a/koloskova19a.pdf},
  abstract     = {We consider decentralized stochastic optimization with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a fixed communication graph. To address the communication bottleneck, the nodes compress (e.g. quantize or sparsify) their model updates. We cover both unbiased and biased compression operators with quality denoted by \delta <= 1 (\delta=1 meaning no compression). We (i) propose a novel gossip-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1/(nT) + 1/(T \rho^2 \delta)^2) for strongly convex objectives, where T denotes the number of iterations and \rho the eigengap of the connectivity matrix. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the average consensus problem that converges in time O(1/(\rho^2\delta) \log (1/\epsilon)) for accuracy \epsilon > 0. This is (up to our knowledge) the first gossip algorithm that supports arbitrary compressed messages for \delta > 0 and still exhibits linear convergence. We (iii) show in experiments that both of our algorithms do outperform the respective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes.}
}
@inproceedings{lin2018deep,
  title        = {Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
  author       = {Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
  year         = 2018,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=SkhQHMW0W}
}
@inproceedings{localsgd_first,
  title        = {Parallelized Stochastic Gradient Descent},
  author       = {Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},
  year         = 2010,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 23,
  pages        = {2595--2603},
  url          = {https://proceedings.neurips.cc/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
  editor       = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta}
}
@article{survey_distributed,
  title        = {A Survey on Distributed Machine Learning},
  author       = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
  year         = 2020,
  month        = mar,
  journal      = {ACM Comput. Surv.},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = 53,
  number       = 2,
  doi          = {10.1145/3377454},
  issn         = {0360-0300},
  url          = {https://doi.org/10.1145/3377454},
  issue_date   = {June 2020},
  abstract     = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
  articleno    = 30,
  numpages     = 33,
  keywords     = {Distributed machine learning, distributed systems}
}
@inproceedings{li2019speeding,
  title        = {Speeding up deep learning with transient servers},
  author       = {Li, Shijian and Walls, Robert J and Xu, Lijie and Guo, Tian},
  year         = 2019,
  booktitle    = {2019 IEEE International Conference on Autonomic Computing (ICAC)},
  pages        = {125--135},
  organization = {IEEE}
}
@inproceedings{zhang2020machine,
  title        = {Machine learning on volatile instances},
  author       = {Zhang, Xiaoxi and Wang, Jianyu and Joshi, Gauri and Joe-Wong, Carlee},
  year         = 2020,
  booktitle    = {IEEE INFOCOM 2020-IEEE Conference on Computer Communications},
  pages        = {139--148},
  organization = {IEEE}
}
@inproceedings{proteus,
  title        = {Proteus: Agile ML Elasticity through Tiered Reliability in Dynamic Resource Markets},
  author       = {Harlap, Aaron and Tumanov, Alexey and Chung, Andrew and Ganger, Gregory R. and Gibbons, Phillip B.},
  year         = 2017,
  booktitle    = {Proceedings of the Twelfth European Conference on Computer Systems},
  location     = {Belgrade, Serbia},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {EuroSys '17},
  pages        = {589–604},
  doi          = {10.1145/3064176.3064182},
  isbn         = 9781450349383,
  url          = {https://doi.org/10.1145/3064176.3064182},
  abstract     = {Many shared computing clusters allow users to utilize excess idle resources at lower cost or priority, with the proviso that some or all may be taken away at any time. But, exploiting such dynamic resource availability and the often fluctuating markets for them requires agile elasticity and effective acquisition strategies. Proteus aggressively exploits such transient revocable resources to do machine learning (ML) cheaper and/or faster. Its parameter server framework, AgileML, efficiently adapts to bulk additions and revocations of transient machines, through a novel 3-stage active-backup approach, with minimal use of more costly non-transient resources. Its BidBrain component adaptively allocates resources from multiple EC2 spot markets to minimize average cost per work as transient resource availability and cost change over time. Our evaluations show that Proteus reduces cost by 85% relative to non-transient pricing, and by 43% relative to previous approaches, while simultaneously reducing runtimes by up to 37%.},
  numpages     = 16
}
@article{zero_ssd,
  title        = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
  author       = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  year         = 2021,
  journal      = {arXiv preprint arXiv:2104.07857}
}
@misc{lin2020multinode,
  title        = {Multi-node Bert-pretraining: Cost-efficient Approach},
  author       = {Jiahuang Lin and Xin Li and Gennady Pekhimenko},
  year         = 2020,
  eprint       = {2008.00177},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@article{Lepikhin2020GShardSG,
  title        = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author       = {Dmitry Lepikhin and H. Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Y. Huang and M. Krikun and Noam Shazeer and Z. Chen},
  year         = 2020,
  journal      = {ArXiv},
  volume       = {abs/2006.16668}
}
@inproceedings{refined_laser,
  title        = {A Refined Laser Method and Faster Matrix Multiplication},
  author       = {Josh Alman and Virginia Vassilevska Williams},
  year         = 2021,
  booktitle    = {SODA}
}
@article{strassen_reloaded,
  title        = {Strassen’s Algorithm Reloaded on GPUs},
  author       = {Huang, Jianyu and Yu, Chenhan D. and Geijn, Robert A. van de},
  year         = 2020,
  month        = mar,
  journal      = {ACM Trans. Math. Softw.},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = 46,
  number       = 1,
  doi          = {10.1145/3372419},
  issn         = {0098-3500},
  url          = {https://doi.org/10.1145/3372419},
  issue_date   = {April 2020},
  abstract     = {Conventional Graphics Processing Unit (GPU) implementations of Strassen’s algorithm (Strassen) rely on the existing high-performance matrix multiplication (gemm), trading space for time. As a result, such approaches can only achieve practical speedup for relatively large, “squarish” matrices due to the extra memory overhead, and their usages are limited due to the considerable workspace. We present novel Strassen primitives for GPUs that can be composed to generate a family of Strassen algorithms. Our algorithms utilize both the memory and thread hierarchies on GPUs, reusing shared memory and register files inherited from gemm, fusing additional operations, and avoiding extra workspace. We further exploit intra- and inter-kernel parallelism by batching, streaming, and employing atomic operations. We develop a performance model for NVIDIA Volta GPUs to select the appropriate blocking parameters and predict the performance for gemm and Strassen. Overall, our 1-level Strassen can achieve up to 1.11\texttimes{} speedup with a crossover point as small as 1,536 compared to cublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level Strassen can achieve 1.19\texttimes{} speedup with a crossover point at 7,680.},
  articleno    = 1,
  numpages     = 22,
  keywords     = {Volta, GEMM, high-performance computing, GPU, Strassen, linear algebra, matrix multiplication, performance optimization}
}
@article{coppersmith_winograd,
  title        = {Matrix multiplication via arithmetic progressions},
  author       = {Don Coppersmith and Shmuel Winograd},
  year         = 1990,
  journal      = {Journal of Symbolic Computation},
  volume       = 9,
  number       = 3,
  pages        = {251--280},
  doi          = {https://doi.org/10.1016/S0747-7171(08)80013-2},
  issn         = {0747-7171},
  url          = {https://www.sciencedirect.com/science/article/pii/S0747717108800132},
  note         = {Computational algebraic complexity editorial},
  abstract     = {We present a new method for accelerating matrix multiplication asymptotically. Thiswork builds on recent ideas of Volker Strassen, by using a basic trilinear form which is not a matrix product. We make novel use of the Salem-Spencer Theorem, which gives a fairly dense set of integers with no three-term arithmetic progression. Our resulting matrix exponent is 2.376.}
}
@inproceedings{practical_matmul_earlier,
  title        = {Understanding the efficiency of GPU algorithms for matrix-matrix multiplication},
  author       = {Fatahalian, Kayvon and Sugerman, Jeremy and Hanrahan, Pat},
  year         = 2004,
  month        = {01},
  journal      = {HWWS '04: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware},
  pages        = {133--137},
  doi          = {10.1145/1058129.1058148}
}
@inproceedings{practical_matmul_best,
  title        = {Matrix Multiplication on High-Density Multi-GPU Architectures: Theoretical and Experimental Investigations},
  author       = {Zhang, Peng and Gao, Yuxiang},
  year         = 2015,
  month        = {06},
  journal      = {Lecture Notes in Computer Science},
  volume       = 9137,
  pages        = {17--30},
  doi          = {10.1007/978-3-319-20119-1_2}
}
@inproceedings{beyond_data_and_model,
  title        = {Beyond Data and Model Parallelism for Deep Neural Networks.},
  author       = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  year         = 2019,
  booktitle    = {Proceedings of Machine Learning and Systems},
  volume       = 1,
  pages        = {1--13},
  url          = {https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf},
  editor       = {A. Talwalkar and V. Smith and M. Zaharia}
}
@inproceedings{sgpush,
  title        = {Stochastic Gradient Push for Distributed Deep Learning},
  author       = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  year         = 2019,
  month        = {09--15 Jun},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 97,
  pages        = {344--353},
  url          = {http://proceedings.mlr.press/v97/assran19a.html},
  editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pdf          = {http://proceedings.mlr.press/v97/assran19a/assran19a.pdf},
  abstract     = {Distributed data-parallel algorithms aim to accelerate the training of deep neural networks by parallelizing the computation of large mini-batch gradient updates across multiple nodes. Approaches that synchronize nodes using exact distributed averaging (e.g., via AllReduce) are sensitive to stragglers and communication delays. The PushSum gossip algorithm is robust to these issues, but only performs approximate distributed averaging. This paper studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient updates. We prove that SGP converges to a stationary point of smooth, non-convex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We empirically validate the performance of SGP on image classification (ResNet-50, ImageNet) and machine translation (Transformer, WMT’16 En-De) workloads.}
}
@inproceedings{zeno,
  title        = {Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance},
  author       = {Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  year         = 2019,
  month        = {09--15 Jun},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 97,
  pages        = {6893--6901},
  url          = {http://proceedings.mlr.press/v97/xie19b.html},
  editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pdf          = {http://proceedings.mlr.press/v97/xie19b/xie19b.pdf},
  abstract     = {We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes previous results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to suspect workers that are potentially defective. Since this is likely to lead to false positives, we use a ranking-based preference mechanism. We prove the convergence of SGD for non-convex problems under these scenarios. Experimental results show that Zeno outperforms existing approaches.}
}
@article{puigcerver2020scalable,
  title        = {Scalable transfer learning with expert models},
  author       = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Renggli, Cedric and Pinto, Andr{\'e} Susano and Gelly, Sylvain and Keysers, Daniel and Houlsby, Neil},
  year         = 2020,
  journal      = {arXiv preprint arXiv:2009.13239}
}
@article{lp_relaxation_largescale,
  title        = {{Outline of an algorithm for integer solutions to linear programs}},
  author       = {Ralph E. Gomory},
  year         = 1958,
  journal      = {Bulletin of the American Mathematical Society},
  publisher    = {American Mathematical Society},
  volume       = 64,
  number       = 5,
  pages        = {275 -- 278},
  doi          = {bams/1183522679},
  url          = {https://doi.org/}
}
@inproceedings{secure_aggregation,
  title        = {Practical Secure Aggregation  for Privacy-Preserving Machine Learning},
  author       = {Aaron Segal and Antonio Marcedone and Benjamin Kreuter and Daniel Ramage and H. Brendan McMahan and Karn Seth and K. A. Bonawitz and Sarvar Patel and Vladimir Ivanov},
  year         = 2017,
  booktitle    = {CCS},
  url          = {https://eprint.iacr.org/2017/281.pdf}
}
@misc{fed_google1,
  title        = {Federated Learning for Mobile Keyboard Prediction},
  author       = {Andrew Hard and Chloé M Kiddon and Daniel Ramage and Francoise Beaufays and Hubert Eichner and Kanishka Rao and Rajiv Mathews and Sean Augenstein},
  year         = 2018,
  url          = {https://arxiv.org/abs/1811.03604}
}
@misc{fed_google2,
  title        = {Applied Federated Learning: Improving Google Keyboard Query Suggestions},
  author       = {Timothy Yang and Galen Andrew and Hubert Eichner and Haicheng Sun and Wei Li and Nicholas Kong and Daniel Ramage and Françoise Beaufays},
  year         = 2018,
  url          = {https://arxiv.org/abs/1812.02903}
}
@article{fed_intel,
  title        = {Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data},
  author       = {Sheller, Micah J. and Edwards, Brandon and Reina, G. Anthony and Martin, Jason and Pati, Sarthak and Kotrotsou, Aikaterini and Milchenko, Mikhail and Xu, Weilin and Marcus, Daniel and Colen, Rivka R. and Bakas, Spyridon},
  year         = 2020,
  month        = {Jul},
  day          = 28,
  journal      = {Scientific Reports},
  volume       = 10,
  number       = 1,
  pages        = 12598,
  doi          = {10.1038/s41598-020-69250-1},
  issn         = {2045-2322},
  url          = {https://doi.org/10.1038/s41598-020-69250-1},
  abstract     = {Several studies underscore the potential of deep learning in identifying complex patterns, leading to diagnostic and prognostic biomarkers. Identifying sufficiently large and diverse datasets, required for training, is a significant challenge in medicine and can rarely be found in individual institutions. Multi-institutional collaborations based on centrally-shared patient data face privacy and ownership challenges. Federated learning is a novel paradigm for data-private multi-institutional collaborations, where model-learning leverages all available data without sharing data between institutions, by distributing the model-training to the data-owners and aggregating their results. We show that federated learning among 10 institutions results in models reaching 99{\%} of the model quality achieved with centralized data, and evaluate generalizability on data from institutions outside the federation. We further investigate the effects of data distribution across collaborating institutions on model quality and learning patterns, indicating that increased access to data through data private multi-institutional collaborations can benefit model quality more than the errors introduced by the collaborative method. Finally, we compare with other collaborative-learning approaches demonstrating the superiority of federated learning, and discuss practical implementation considerations. Clinical adoption of federated learning is expected to lead to models trained on datasets of unprecedented size, hence have a catalytic impact towards precision/personalized medicine.}
}
@article{Popel2018TrainingTF,
  title        = {Training Tips for the Transformer Model},
  author       = {M. Popel and Ondrej Bojar},
  year         = 2018,
  journal      = {The Prague Bulletin of Mathematical Linguistics},
  volume       = 110,
  pages        = {43--70}
}
@article{gpt2,
  title        = {Language Models are Unsupervised Multitask Learners},
  author       = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year         = 2019
}


@misc{opt,
  doi = {10.48550/ARXIV.2205.01068},
  
  url = {https://arxiv.org/abs/2205.01068},
  
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OPT: Open Pre-trained Transformer Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{yong_adapting,
  doi = {10.48550/ARXIV.2204.04873},
  
  url = {https://arxiv.org/abs/2204.04873},
  
  author = {Yong, Zheng-Xin and Nikoulina, Vassilina},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adapting BigScience Multilingual Model to Unseen Languages},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@article{glam,
  author    = {Nan Du and
               Yanping Huang and
               Andrew M. Dai and
               Simon Tong and
               Dmitry Lepikhin and
               Yuanzhong Xu and
               Maxim Krikun and
               Yanqi Zhou and
               Adams Wei Yu and
               Orhan Firat and
               Barret Zoph and
               Liam Fedus and
               Maarten Bosma and
               Zongwei Zhou and
               Tao Wang and
               Yu Emma Wang and
               Kellie Webster and
               Marie Pellat and
               Kevin Robinson and
               Kathy Meier{-}Hellstern and
               Toju Duke and
               Lucas Dixon and
               Kun Zhang and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen and
               Claire Cui},
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  journal   = {CoRR},
  volume    = {abs/2112.06905},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.06905},
  eprinttype = {arXiv},
  eprint    = {2112.06905},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-06905.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






@article{hyperclova,
  author    = {Boseop Kim and
               HyoungSeok Kim and
               Sang{-}Woo Lee and
               Gichang Lee and
               Dong{-}Hyun Kwak and
               Dong Hyeon Jeon and
               Sunghyun Park and
               Sungju Kim and
               Seonhoon Kim and
               Dongpil Seo and
               Heungsub Lee and
               Minyoung Jeong and
               Sungjae Lee and
               Minsub Kim and
               SukHyun Ko and
               Seokhun Kim and
               Taeyong Park and
               Jinuk Kim and
               Soyoung Kang and
               Na{-}Hyeon Ryu and
               Kang Min Yoo and
               Minsuk Chang and
               Soobin Suh and
               Sookyo In and
               Jinseong Park and
               Kyungduk Kim and
               Hiun Kim and
               Jisu Jeong and
               Yong Goo Yeo and
               Donghoon Ham and
               Dongju Park and
               Min Young Lee and
               Jaewook Kang and
               Inho Kang and
               Jung{-}Woo Ha and
               Woo{-}Myoung Park and
               Nako Sung},
  title     = {What Changes Can Large-scale Language Models Bring? Intensive Study
               on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers},
  journal   = {CoRR},
  volume    = {abs/2109.04650},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.04650},
  eprinttype = {arXiv},
  eprint    = {2109.04650},
  timestamp = {Thu, 09 Jun 2022 10:52:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-04650.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{pangua,
  author    = {Wei Zeng and
               Xiaozhe Ren and
               Teng Su and
               Hui Wang and
               Yi Liao and
               Zhiwei Wang and
               Xin Jiang and
               ZhenZhang Yang and
               Kaisheng Wang and
               Xiaoda Zhang and
               Chen Li and
               Ziyan Gong and
               Yifan Yao and
               Xinjing Huang and
               Jun Wang and
               Jianfeng Yu and
               Qi Guo and
               Yue Yu and
               Yan Zhang and
               Jin Wang and
               Hengtao Tao and
               Dasen Yan and
               Zexuan Yi and
               Fang Peng and
               Fangqing Jiang and
               Han Zhang and
               Lingfeng Deng and
               Yehong Zhang and
               Zhe Lin and
               Chao Zhang and
               Shaojie Zhang and
               Mingyue Guo and
               Shanzhi Gu and
               Gaojun Fan and
               Yaowei Wang and
               Xuefeng Jin and
               Qun Liu and
               Yonghong Tian},
  title     = {PanGu-{\(\alpha\)}: Large-scale Autoregressive Pretrained Chinese
               Language Models with Auto-parallel Computation},
  journal   = {CoRR},
  volume    = {abs/2104.12369},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.12369},
  eprinttype = {arXiv},
  eprint    = {2104.12369},
  timestamp = {Wed, 01 Sep 2021 15:28:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-12369.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{gopher,
  author    = {Jack W. Rae and
               Sebastian Borgeaud and
               Trevor Cai and
               Katie Millican and
               Jordan Hoffmann and
               H. Francis Song and
               John Aslanides and
               Sarah Henderson and
               Roman Ring and
               Susannah Young and
               Eliza Rutherford and
               Tom Hennigan and
               Jacob Menick and
               Albin Cassirer and
               Richard Powell and
               George van den Driessche and
               Lisa Anne Hendricks and
               Maribeth Rauh and
               Po{-}Sen Huang and
               Amelia Glaese and
               Johannes Welbl and
               Sumanth Dathathri and
               Saffron Huang and
               Jonathan Uesato and
               John Mellor and
               Irina Higgins and
               Antonia Creswell and
               Nat McAleese and
               Amy Wu and
               Erich Elsen and
               Siddhant M. Jayakumar and
               Elena Buchatskaya and
               David Budden and
               Esme Sutherland and
               Karen Simonyan and
               Michela Paganini and
               Laurent Sifre and
               Lena Martens and
               Xiang Lorraine Li and
               Adhiguna Kuncoro and
               Aida Nematzadeh and
               Elena Gribovskaya and
               Domenic Donato and
               Angeliki Lazaridou and
               Arthur Mensch and
               Jean{-}Baptiste Lespiau and
               Maria Tsimpoukelli and
               Nikolai Grigorev and
               Doug Fritz and
               Thibault Sottiaux and
               Mantas Pajarskas and
               Toby Pohlen and
               Zhitao Gong and
               Daniel Toyama and
               Cyprien de Masson d'Autume and
               Yujia Li and
               Tayfun Terzi and
               Vladimir Mikulik and
               Igor Babuschkin and
               Aidan Clark and
               Diego de Las Casas and
               Aurelia Guy and
               Chris Jones and
               James Bradbury and
               Matthew Johnson and
               Blake A. Hechtman and
               Laura Weidinger and
               Iason Gabriel and
               William S. Isaac and
               Edward Lockhart and
               Simon Osindero and
               Laura Rimell and
               Chris Dyer and
               Oriol Vinyals and
               Kareem Ayoub and
               Jeff Stanway and
               Lorrayne Bennett and
               Demis Hassabis and
               Koray Kavukcuoglu and
               Geoffrey Irving},
  title     = {Scaling Language Models: Methods, Analysis {\&} Insights from
               Training Gopher},
  journal   = {CoRR},
  volume    = {abs/2112.11446},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.11446},
  eprinttype = {arXiv},
  eprint    = {2112.11446},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-11446.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{gated_improve,
  title        = {{GLU} Variants Improve Transformer},
  author       = {Noam Shazeer},
  year         = 2020,
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype   = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@book{square_cube,
  title        = {Discorsi e dimostrazioni matematiche intorno a due nuove scienze},
  author       = {Galilei Galileo},
  year         = 1638
}
@book{mechanics,
  title        = {How Mechanics Shaped the Modern World},
  author       = {David H. Allen},
  year         = 2013,
  isbn         = 9783319017013
}
@inbook{fed_nvidia,
  title        = {Privacy-Preserving Federated Brain Tumour Segmentation},
  author       = {Wenqi Li and Fausto Milletar{\`i} and Daguang Xu and Nicola Rieke and Jonny Hancox and Wentao Zhu and Maximilian Baust and Yan Cheng and S{\'e}bastien Ourselin and Cardoso, {M. Jorge} and Andrew Feng},
  year         = 2019,
  month        = jan,
  day          = 1,
  booktitle    = {Machine Learning in Medical Imaging - 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Proceedings},
  publisher    = {SPRINGER},
  series       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  pages        = {133--141},
  doi          = {10.1007/978-3-030-32692-0_16},
  isbn         = 9783030326913,
  note         = {10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019 held in conjunction with the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019 ; Conference date: 13-10-2019 Through 13-10-2019},
  abstract     = {Due to medical data privacy regulations, it is often infeasible to collect and share patient data in a centralised data lake. This poses challenges for training machine learning algorithms, such as deep convolutional networks, which often require large numbers of diverse training examples. Federated learning sidesteps this difficulty by bringing code to the patient data owners and only sharing intermediate model training updates among them. Although a high-accuracy model could be achieved by appropriately aggregating these model updates, the model shared could indirectly leak the local training examples. In this paper, we investigate the feasibility of applying differential-privacy techniques to protect the patient data in a federated learning setup. We implement and evaluate practical federated learning systems for brain tumour segmentation on the BraTS dataset. The experimental results show that there is a trade-off between model performance and privacy protection costs.},
  language     = {English},
  editor       = {Heung-Il Suk and Mingxia Liu and Chunfeng Lian and Pingkun Yan}
}
@inproceedings{federatedlearningatscale,
  title        = {Towards Federated Learning at Scale: System Design},
  author       = {K. A. Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloé M Kiddon and Jakub Konečný and Stefano Mazzocchi and Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
  year         = 2019,
  booktitle    = {SysML 2019},
  url          = {https://arxiv.org/abs/1902.01046},
  note         = {To appear}
}
,
  note = {Accessed: 2020-2-10}
}
@inproceedings{dai2019transformer,
  title        = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author       = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  year         = 2019,
  booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages        = {2978--2988}
}
@article{wikitext103,
  title        = {Pointer Sentinel Mixture Models},
  author       = {Stephen Merity and Caiming Xiong and James Bradbury and R. Socher},
  year         = 2017,
  journal      = {ArXiv},
  volume       = {abs/1609.07843}
}
@article{datasets,
  title        = {Datasets},
  author       = {Thomas Wolf and Quentin Lhoest and Patrick von Platen and Yacine Jernite and Mariama Drame and Julien Plu and Julien Chaumond and Clement Delangue and Clara Ma and Abhishek Thakur and Suraj Patil and Joe Davison and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angie McMillan-Major and Simon Brandeis and Sylvain Gugger and François Lagunas and Lysandre Debut and Morgan Funtowicz and Anthony Moi and Sasha Rush and Philipp Schmidd and Pierric Cistac and Victor Muštar and Jeff Boudier and Anna Tordjmann},
  year         = 2020,
  journal      = {GitHub. Note: https://github.com/huggingface/datasets},
  volume       = 1
}
@article{accelerate,
  title        = {Accelerate: Run your raw PyTorch training script on any kind of device.},
  author       = {Hugging~Face and contributors},
  year         = 2020,
  journal      = {GitHub. Note: https://github.com/huggingface/datasets},
  volume       = 1
}

@misc{tfew,
  doi = {10.48550/ARXIV.2205.05638},
  
  url = {https://arxiv.org/abs/2205.05638},
  
  author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}




@article{wikitext2,
  title        = {Wikitext-2},
  author       = {Stephen Merity et al., 2016},
  year         = {},
  journal      = {},
  url          = {https://arxiv.org/abs/1609.07843},
  keywords     = {fastai},
  license      = {},
  abstract     = {A subset of Wikitext-103; useful for testing language model training on smaller datasets.},
  superseded   = {},
  terms        = {}
}
@misc{gpt3costlambda,
  title        = {Demystifying GPT-3 Language Model: A Technical Overview},
  author       = {Chuan Li},
  year         = 2020,
  note         = {"\url{https://lambdalabs.com/blog/demystifying-gpt-3}"}
}
@book{circuit-relay,
  title        = {Circuit Relay},
  author    = {ProtocolLabs},
  year         = 2020,
  note         = {"\url{https://docs.libp2p.io/concepts/circuit-relay/}"}
}
@misc{ga102-datasheet,
  title        = {NVIDIA Ampere GA102 GPU Architecture},
  author       = {NVIDIA},
  year         = 2020,
  url          = {https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf}
}
@misc{bloom,
  title        = {BigScience Large Open-science Open-access Multilingual Language Model},
  author       = {BigScience},
  year         = 2022,
  note         = {"\url{https://huggingface.co/bigscience/bloom}"}
}

@misc{nvidia_perf,
  title        = {NVIDIA Data Center Deep Learning Product Performance},
  author       = {NVIDIA},
  note         = {"\url{https://developer.nvidia.com/deep-learning-performance-training-inference}", accessed at 2021.02.03}
}
@misc{dettmerswikitext2,
  author       = {Tim Dettmers},
  note         = {https://github.com/TimDettmers/transformer-xl/tree/wikitext2}
}
@misc{yalm,
  title        = {YaLM 100B},
  author       = {Michael Khrushchev and Ruslan Vasilev and Nikolay Zinov and Alexey Petrov and Yandex},
  year         = 2022,
  note         = {\url{"https://huggingface.co/yandex/yalm-100b"}}
}
@misc{gpt-neox-20b,
  doi = {10.48550/ARXIV.2204.06745},
  
  url = {https://arxiv.org/abs/2204.06745},
  
  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{mlperf-arxiv,
  title        = {MLPerf Training Benchmark},
  author       = {Peter Mattson and Christine Cheng and Cody Coleman and Greg Diamos and Paulius Micikevicius and David A. Patterson and Hanlin Tang and Gu{-}Yeon Wei and Peter Bailis and Victor Bittorf and David Brooks and Dehao Chen and Debojyoti Dutta and Udit Gupta and Kim M. Hazelwood and Andrew Hock and Xinyuan Huang and Bill Jia and Daniel Kang and David Kanter and Naveen Kumar and Jeffery Liao and Guokai Ma and Deepak Narayanan and Tayo Oguntebi and Gennady Pekhimenko and Lillian Pentecost and Vijay Janapa Reddi and Taylor Robie and Tom St. John and Carole{-}Jean Wu and Lingjie Xu and Cliff Young and Matei Zaharia},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1910.01500},
  url          = {http://arxiv.org/abs/1910.01500},
  archiveprefix = {arXiv},
  eprint       = {1910.01500},
  timestamp    = {Mon, 04 Nov 2019 08:16:51 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01500.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{variability_azure,
  title        = {On Network Throughput Variability in Microsoft Azure Cloud},
  author       = {V. {Persico} and P. {Marchetta} and A. {Botta} and A. {Pescape}},
  year         = 2015,
  booktitle    = {2015 IEEE Global Communications Conference (GLOBECOM)},
  volume       = {},
  number       = {},
  pages        = {1--6},
  doi          = {10.1109/GLOCOM.2015.7416997}
}
@article{variability_aws,
  title        = {Measuring network throughput in the cloud: The case of Amazon EC2},
  author       = {Valerio Persico and Pietro Marchetta and Alessio Botta and Antonio Pescapè},
  year         = 2015,
  journal      = {Computer Networks},
  volume       = 93,
  pages        = {408--422},
  doi          = {https://doi.org/10.1016/j.comnet.2015.09.037},
  issn         = {1389-1286},
  url          = {http://www.sciencedirect.com/science/article/pii/S138912861500362X},
  note         = {Cloud Networking and Communications II},
  keywords     = {Cloud networking monitoring and measurement, Cloud networking performance, Cloud network throughput},
  abstract     = {Cloud providers employ sophisticated virtualization techniques and strategies for sharing resources among a large number of largely uncoordinated and mutually untrusted customers. The shared networking environment, in particular, dictates the need for mechanisms to partition network resources among virtual machines. At the same time, the performance of applications deployed over these virtual machines may be heavily impacted by the performance of the underlying network, and therefore by such mechanisms. Nevertheless, due to security and commercial reasons, providers rarely provide detailed information on network organization, performance, and mechanisms employed to regulate it. In addition, the scientific literature only provides a blurred image of the network performance inside the cloud. The few available pioneer works marginally focus on this aspect, use different methodologies, operate in few limited scenarios, or report conflicting results. In this paper, we present a detailed analysis of the performance of the internal network of Amazon EC2, performed by adopting a non-cooperative experimental evaluation approach (i.e. not relying on provider support). Our aim is to provide a quantitative assessment of the networking performance as a function of the several variables available, such as geographic region, resource price or size. We propose a detailed methodology to perform this kind of analysis, which we believe is essential in a such complex and dynamic environment. During this analysis we have discovered and analyzed the limitations enforced by Amazon over customer traffic in terms of maximum throughput allowed. Thanks to our work it is possible to understand how the complex mechanisms enforced by the provider in order to manage its infrastructure impact the performance perceived by the cloud customers and potentially tamper with monitoring and controlling approaches previously proposed in literature. Leveraging our knowledge of the bandwidth-limiting mechanisms, we then present a clear picture of the maximum throughput achievable in Amazon EC2 network, shedding light on when and how such maximum throughput can be achieved and at which cost.}
}
@incollection{Bengio+chapter2007,
  title        = {Scaling Learning Algorithms Towards {AI}},
  author       = {Bengio, Yoshua and LeCun, Yann},
  year         = 2007,
  booktitle    = {Large Scale Kernel Machines},
  publisher    = {MIT Press}
}
@article{Hinton06,
  title        = {A Fast Learning Algorithm for Deep Belief Nets},
  author       = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
  year         = 2006,
  journal      = {Neural Computation},
  volume       = 18,
  pages        = {1527--1554}
}
@book{goodfellow2016deep,
  title        = {Deep learning},
  author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  year         = 2016,
  publisher    = {MIT Press},
  volume       = 1
}
@article{Hendrycks2019NaturalAE,
  title        = {Natural Adversarial Examples},
  author       = {Dan Hendrycks and Kevin Keliang Zhao and Steven Basart and Jacob Steinhardt and Dawn Xiaodong Song},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1907.07174}
}
@inproceedings{Ott2019fairseqAF,
  title        = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author       = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  year         = 2019,
  booktitle    = {NAACL-HLT}
}
@article{hern2018facebook,
  title        = {Facebook translates ``good morning'' into ``attack them'', leading to arrest},
  author       = {Alex Hern},
  year         = 2018,
  journal      = {The Guardian}
}
@article{He2015DeepRL,
  title        = {Deep Residual Learning for Image Recognition},
  author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year         = 2015,
  journal      = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {770--778}
}
@article{Papernot2018DeepKN,
  title        = {Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning},
  author       = {Nicolas Papernot and Patrick D. McDaniel},
  year         = 2018,
  journal      = {ArXiv},
  volume       = {abs/1803.04765}
}
@article{Garnelo2018ConditionalNP,
  title        = {Conditional Neural Processes},
  author       = {Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo Jimenez Rezende and S. M. Ali Eslami},
  year         = 2018,
  journal      = {ArXiv},
  volume       = {abs/1807.01613}
}
@inproceedings{Wallace2019UniversalAT,
  title        = {Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author       = {Eric Wallace and Feng Shi and Nikhil Kandpal and Matt Gardner and Sameer Singh},
  year         = 2019
}
@article{Szegedy2013IntriguingPO,
  title        = {Intriguing properties of neural networks},
  author       = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
  year         = 2013,
  journal      = {CoRR},
  volume       = {abs/1312.6199}
}
@article{Yuan2017AdversarialEA,
  title        = {Adversarial Examples: Attacks and Defenses for Deep Learning},
  author       = {Xiaoyong Yuan and Pan He and Qile Zhu and Xiaolin Li},
  year         = 2017,
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  volume       = 30,
  pages        = {2805--2824}
}
@inproceedings{Ebrahimi2017HotFlipWA,
  title        = {HotFlip: White-Box Adversarial Examples for Text Classification},
  author       = {Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
  year         = 2017,
  booktitle    = {ACL}
}
@article{schmidthuber,
  title        = {First superhuman visual pattern recognition},
  author       = {D. C. Ciresan, U. Meier, J. Schmidhuber},
  year         = 2011,
  journal      = {IJCNN}
}
@article{microsoft_mt_parity,
  title        = {Achieving Human Parity on Automatic Chinese to English News Translation},
  author       = {Hany Hassan and Anthony Aue and Chang Chen and Vishal Chowdhary and Jonathan R. Clark and Christian Federmann and Xuedong Huang and Marcin Junczys-Dowmunt and William Lewis and Mu Li and Shujie Liu and T. M. Liu and Renqian Luo and Arul Menezes and Tao Qin and Frank Seide and Xu Tan and Fei Tian and Lijun Wu and Shuangzhi Wu and Yingce Xia and Dongdong Zhang and Zhirui Zhang and Ming Zhou},
  year         = 2018,
  journal      = {ArXiv},
  volume       = {abs/1803.05567}
}
@article{Silver2016MasteringTG,
  title        = {Mastering the game of Go with deep neural networks and tree search},
  author       = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  year         = 2016,
  journal      = {Nature},
  volume       = 529,
  pages        = {484--489}
}
@inproceedings{Singh2013OpticalCR,
  title        = {Optical Character Recognition Techniques: A survey},
  author       = {Sukhpreet Singh},
  year         = 2013
}
@inproceedings{maml,
  title        = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author       = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  year         = 2017,
  booktitle    = {ICML}
}
@article{reptile,
  title        = {On First-Order Meta-Learning Algorithms},
  author       = {Alex Nichol and Joshua Achiam and John Schulman},
  year         = 2018,
  journal      = {ArXiv},
  volume       = {abs/1803.02999}
}
@article{learning2learn,
  title        = {Learning to learn by gradient descent by gradient descent},
  author       = {Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  year         = 2016,
  journal      = {ArXiv},
  volume       = {abs/1606.04474}
}
@article{elasticweightcolsolidation,
  title        = {Overcoming catastrophic forgetting in neural networks},
  author       = {James Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
  year         = 2016,
  journal      = {Proceedings of the National Academy of Sciences of the United States of America},
  volume       = {114 13},
  pages        = {3521--3526}
}
@article{rehearsal,
  title        = {Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
  author       = {Anthony V. Robins},
  year         = 1995,
  journal      = {Connect. Sci.},
  volume       = 7,
  pages        = {123--146}
}
@article{Houthooft2018EvolvedPG,
  title        = {Evolved Policy Gradients},
  author       = {Rein Houthooft and Yuhua Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},
  year         = 2018,
  journal      = {ArXiv},
  volume       = {abs/1802.04821}
}
@article{withoutgd,
  title        = {Learning to learn by gradient descent by gradient descent},
  author       = {Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  year         = 2016,
  journal      = {ArXiv},
  volume       = {abs/1606.04474}
}
@article{Lin2019ConditionalCF,
  title        = {Conditional Computation for Continual Learning},
  author       = {Min Lin and Jie Fu and Yoshua Bengio},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1906.06635}
}
@article{clmetalearning1,
  title        = {Task Agnostic Continual Learning via Meta Learning},
  author       = {Xu He and Jakub Sygnowski and Alexandre Galashov and Andrei A. Rusu and Yee Whye Teh and Razvan Pascanu},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1906.05201}
}
@article{clmetalearning2,
  title        = {Meta-Learning Representations for Continual Learning},
  author       = {Khurram Javed and Martha White},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1905.12588}
}
@article{conv_first,
  title        = {{N}eocognitron: {A} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  author       = {Fukushima, Kunihiko},
  year         = 1980,
  journal      = {Biological Cybernetics},
  volume       = 36,
  pages        = {193--202},
  added-at     = {2008-03-11T14:52:34.000+0100},
  biburl       = {https://www.bibsonomy.org/bibtex/29ecd878c4827c46dab6b9622cfa00072/idsia},
  citeulike-article-id = 2376719,
  interhash    = {303975e6400e477e91c91e7dc2c47544},
  intrahash    = {9ecd878c4827c46dab6b9622cfa00072},
  keywords     = {nn},
  priority     = 2,
  timestamp    = {2008-03-11T15:04:22.000+0100}
}
@article{densenet,
  title        = {Densely Connected Convolutional Networks},
  author       = {Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
  year         = 2016,
  journal      = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {2261--2269}
}
@article{adagrad,
  title        = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author       = {John C. Duchi and Elad Hazan and Yoram Singer},
  year         = 2010,
  journal      = {J. Mach. Learn. Res.},
  volume       = 12,
  pages        = {2121--2159}
}
@article{adadelta,
  title        = {ADADELTA: An Adaptive Learning Rate Method},
  author       = {Matthew D. Zeiler},
  year         = 2012,
  journal      = {ArXiv},
  volume       = {abs/1212.5701}
}
@article{superconvergence,
  title        = {Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},
  author       = {Leslie N. Smith and Nicholay Topin},
  year         = 2017,
  journal      = {ArXiv},
  volume       = {abs/1708.07120}
}
@inproceedings{adversarial_training,
  title        = {Explaining and Harnessing Adversarial Examples},
  author       = {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
  year         = 2015,
  booktitle    = {International Conference on Learning Representations},
  url          = {http://arxiv.org/abs/1412.6572}
}
@inproceedings{signsgd,
  title        = {SIGNSGD: Compressed Optimisation for Non-Convex Problems.},
  author       = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  year         = 2018,
  booktitle    = {ICML},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {559--568},
  url          = {http://dblp.uni-trier.de/db/conf/icml/icml2018.html#BernsteinWAA18},
  added-at     = {2019-04-03T00:00:00.000+0200},
  biburl       = {https://www.bibsonomy.org/bibtex/2409dcbd9da24821e21edb290debe9944/dblp},
  crossref     = {conf/icml/2018},
  editor       = {Dy, Jennifer G. and Krause, Andreas},
  ee           = {http://proceedings.mlr.press/v80/bernstein18a.html},
  interhash    = {a579bb8583ea2fdba022637317600f88},
  intrahash    = {409dcbd9da24821e21edb290debe9944},
  keywords     = {dblp},
  timestamp    = {2019-04-04T11:43:21.000+0200}
}
@misc{rmsprop,
  title        = {{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author       = {Tieleman, T. and Hinton, G.},
  year         = 2012,
  howpublished = {COURSERA: Neural Networks for Machine Learning}
}
@article{cifar,
  title        = {CIFAR-10 (Canadian Institute for Advanced Research)},
  author       = {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
  year         = {},
  journal      = {},
  url          = {http://www.cs.toronto.edu/~kriz/cifar.html},
  abstract     = {
    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

    The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.
  },
  keywords     = {Dataset},
  terms        = {}
}
@inproceedings{GAN,
  title        = {Generative Adversarial Nets},
  author       = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year         = 2014,
  booktitle    = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  location     = {Montreal, Canada},
  publisher    = {MIT Press},
  address      = {Cambridge, MA, USA},
  series       = {NIPS'14},
  pages        = {2672–2680},
  numpages     = 9
}
@article{trainingtips,
  title        = {Training Tips for the Transformer Model},
  author       = {Popel, Martin and Bojar, Ondřej},
  year         = 2018,
  month        = {03},
  journal      = {The Prague Bulletin of Mathematical Linguistics},
  volume       = 110,
  pages        = {},
  doi          = {10.2478/pralin-2018-0002}
}
@inproceedings{Li2017VisualizingTL,
  title        = {Visualizing the Loss Landscape of Neural Nets},
  author       = {Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
  year         = 2017,
  booktitle    = {NeurIPS}
}
@inproceedings{tsne,
  title        = {Visualizing Data using t-SNE},
  author       = {Laurens van der Maaten and Geoffrey E. Hinton},
  year         = 2008
}
@article{backprop_rnn,
  title        = {Learning representations by back-propagating errors},
  author       = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  year         = 1986,
  journal      = {Nature},
  volume       = 323,
  pages        = {533--536}
}
@techreport{lstm,
  title        = {{Long Short-Term Memory}},
  author       = {S. Hochreiter and J. Schmidhuber},
  year         = 1995,
  number       = {FKI-207-95},
  note         = {Revised 1996 (see www.idsia.ch/\~{}juergen, www7.informatik.tu-muenchen.de/\~{}hochreit)},
  institution  = {Fakult\"{a}t f\"{u}r Informatik, Technische Universit\"{a}t M\"{u}nchen}
}
@article{elu,
  title        = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  author       = {Djork-Arn{\'e} Clevert and Thomas Unterthiner and Sepp Hochreiter},
  year         = 2015,
  journal      = {CoRR},
  volume       = {abs/1511.07289}
}
@article{Hinton2015DistillingTK,
  title        = {Distilling the Knowledge in a Neural Network},
  author       = {Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  year         = 2015,
  journal      = {ArXiv},
  volume       = {abs/1503.02531}
}
@inproceedings{iwslt14,
  title        = {Report on the 11 th IWSLT Evaluation Campaign , IWSLT 2014},
  author       = {Mauro Cettolo and Jan Niehues and Sebastian St{\"u}ker and Luisa Bentivogli and Marcello Federico},
  year         = 2015
}
@inproceedings{koehn-etal-2007-moses,
  title        = {{M}oses: Open Source Toolkit for Statistical Machine Translation},
  author       = {Koehn, Philipp  and Hoang, Hieu  and Birch, Alexandra  and Callison-Burch, Chris  and Federico, Marcello  and Bertoldi, Nicola  and Cowan, Brooke  and Shen, Wade  and Moran, Christine  and Zens, Richard  and Dyer, Chris  and Bojar, Ond{\v{r}}ej  and Constantin, Alexandra  and Herbst, Evan},
  year         = 2007,
  month        = jun,
  booktitle    = {Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions},
  publisher    = {Association for Computational Linguistics},
  address      = {Prague, Czech Republic},
  pages        = {177--180},
  url          = {https://www.aclweb.org/anthology/P07-2045}
}
@incollection{transformer,
  title        = {Attention is All you Need},
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems 30},
  publisher    = {Curran Associates, Inc.},
  pages        = {5998--6008},
  url          = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{ratcliff1990connectionist,
  title        = {Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author       = {Ratcliff, Roger},
  year         = 1990,
  journal      = {Psychological review},
  publisher    = {American Psychological Association},
  volume       = 97,
  number       = 2,
  pages        = 285
}
@inproceedings{Furlanello2018BornAgainNN,
  title        = {Born-Again Neural Networks},
  author       = {Tommaso Furlanello and Zachary Chase Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar},
  year         = 2018,
  booktitle    = {ICML}
}
@article{inception_v3,
  title        = {Rethinking the Inception Architecture for Computer Vision},
  author       = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jon Shlens and Zbigniew Wojna},
  year         = 2015,
  journal      = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {2818--2826}
}
@inproceedings{mlperf,
  title        = {{MLPerf Training Benchmark}},
  author       = {Peter Mattson and Christine Cheng and Cody Coleman and Greg Diamos and Paulius Micikevicius and David Patterson and Hanlin Tang and Gu-Yeon Wei and Peter Bailis and Victor Bittorf and David Brooks and Dehao Chen and Debojyoti Dutta and Udit Gupta and Kim Hazelwood and Andrew Hock and Xinyuan Huang and Bill Jia and Daniel Kang and David Kanter and Naveen Kumar and Jeffery Liao and Guokai Ma and Deepak Narayanan and Tayo Oguntebi and Gennady Pekhimenko and Lillian Pentecost and Vijay Janapa Reddi and Taylor Robie and Tom St. John and Carole-Jean Wu and Lingjie Xu and Cliff Young and Matei Zaharia},
  year         = 2020,
  booktitle    = {{Proceedings of the 3rd Conference on Machine Learning and Systems (MLSys'20)}}
}
@article{sgd_with_momentum,
  title        = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author       = {Qian, Ning},
  year         = 1999,
  month        = jan,
  journal      = {Neural Netw.},
  publisher    = {Elsevier Science Ltd.},
  address      = {Oxford, UK, UK},
  volume       = 12,
  number       = 1,
  pages        = {145--151},
  issue_date   = {Jan. 1999},
  numpages     = 7,
  keywords     = {critical damping, damped harmonic oscillator, gradient descent learning algorithm, learning rate, momentum, speed of convergence}
}
@inproceedings{adam,
  title        = {Adam: {A} Method for Stochastic Optimization},
  author       = {Diederik P. Kingma and Jimmy Ba},
  year         = 2015,
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015},
  timestamp    = {Fri, 29 Mar 2019 10:36:36 +0100},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bookcorpus,
  title        = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author       = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  year         = 2015,
  booktitle    = {Proceedings of the IEEE international conference on computer vision},
  pages        = {19--27}
}
@inproceedings{reddi2021adaptive,
  title        = {Adaptive Federated Optimization},
  author       = {Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
  year         = 2021,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=LkFG3lB13U5}
}
@inproceedings{chen2020toward,
  title        = {Toward Communication Efficient Adaptive Gradient Method},
  author       = {Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
  year         = 2020,
  booktitle    = {Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
  location     = {Virtual Event, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {FODS '20},
  pages        = {119–128},
  doi          = {10.1145/3412815.3416891},
  isbn         = 9781450381031,
  url          = {https://doi.org/10.1145/3412815.3416891},
  numpages     = 10,
  keywords     = {adaptive method, federated learning, convergence analysis}
}
@inproceedings{pmlr-v97-karimireddy19a,
  title        = {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
  author       = {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  year         = 2019,
  month        = {09--15 Jun},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 97,
  pages        = {3252--3261},
  url          = {http://proceedings.mlr.press/v97/karimireddy19a.html},
  editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pdf          = {http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf},
  abstract     = {Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the biased nature of the sign compression operator. We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory.}
}
@article{scipy,
  title        = {{{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}},
  author       = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year         = 2020,
  journal      = {Nature Methods},
  volume       = 17,
  pages        = {261--272},
  doi          = {10.1038/s41592-019-0686-2},
  adsurl       = {https://rdcu.be/b08Wh}
}
@article{kaplan1974application,
  title        = {Application of programs with maximin objective functions to problems of optimal resource allocation},
  author       = {Kaplan, Seymour},
  year         = 1974,
  journal      = {Operations Research},
  publisher    = {INFORMS},
  volume       = 22,
  number       = 4,
  pages        = {802--807}
}
@article{Joshi2020TheSA,
  title        = {The State and Fate of Linguistic Diversity and Inclusion in the NLP World},
  author       = {Pratik M. Joshi and Sebastin Santy and A. Budhiraja and K. Bali and M. Choudhury},
  year         = 2020,
  journal      = {ArXiv},
  volume       = {abs/2004.09095}
}
@article{Schwartz2020GreenA,
  title        = {Green AI},
  author       = {Roy Schwartz and Jesse Dodge and N. A. Smith and Oren Etzioni},
  year         = 2020,
  journal      = {Communications of the ACM},
  volume       = 63,
  pages        = {54--63}
}
@article{Strubell2019EnergyAP,
  title        = {Energy and Policy Considerations for Deep Learning in NLP},
  author       = {Emma Strubell and Ananya Ganesh and A. McCallum},
  year         = 2019,
  journal      = {ArXiv},
  volume       = {abs/1906.02243}
}
@article{Henderson2020TowardsTS,
  title        = {Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning},
  author       = {Peter Henderson and Jie-Ru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
  year         = 2020,
  journal      = {ArXiv},
  volume       = {abs/2002.05651}
}
@article{Anthony2020CarbontrackerTA,
  title        = {Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author       = {Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  year         = 2020,
  journal      = {ArXiv},
  volume       = {abs/2007.03051}
}
@article{Kline2016HolisticallyET,
  title        = {Holistically evaluating the environmental impacts in modern computing systems},
  author       = {Donald Kline and Nikolas Parshook and Xiaoyu Ge and E. Brunvand and R. Melhem and Panos K. Chrysanthis and A. Jones},
  year         = 2016,
  journal      = {2016 Seventh International Green and Sustainable Computing Conference (IGSC)},
  pages        = {1--8}
}
@article{Bashroush2018ACR,
  title        = {A Comprehensive Reasoning Framework for Hardware Refresh in Data Centers},
  author       = {R. Bashroush},
  year         = 2018,
  journal      = {IEEE Transactions on Sustainable Computing},
  volume       = 3,
  pages        = {209--220}
}
@article{Qiu2020CanFL,
  title        = {Can Federated Learning Save The Planet},
  author       = {Xinchi Qiu and Titouan Parcollet and Daniel J. Beutel and Taner Topal and Akhil Mathur and N. Lane},
  year         = 2020,
  journal      = {arXiv: Learning}
}
@misc{baevski2020wav2vec,
  title        = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author       = {Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
  year         = 2020,
  eprint       = {2006.11477},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@inproceedings{xlmr,
  title        = {Unsupervised Cross-lingual Representation Learning at Scale},
  author       = {Conneau, Alexis  and Khandelwal, Kartikay  and Goyal, Naman  and Chaudhary, Vishrav  and Wenzek, Guillaume  and Guzm{\'a}n, Francisco  and Grave, Edouard  and Ott, Myle  and Zettlemoyer, Luke  and Stoyanov, Veselin},
  year         = 2020,
  month        = jul,
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {8440--8451},
  doi          = {10.18653/v1/2020.acl-main.747},
  url          = {https://www.aclweb.org/anthology/2020.acl-main.747}
}
@misc{switch,
  title        = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author       = {William Fedus and Barret Zoph and Noam Shazeer},
  year         = 2021,
  eprint       = {2101.03961},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@misc{lars,
  title        = {Large Batch Training of Convolutional Networks},
  author       = {Yang You and Igor Gitman and Boris Ginsburg},
  year         = 2017,
  eprint       = {1708.03888},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@misc{lamb,
  title        = {Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
  author       = {Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
  year         = 2020,
  eprint       = {1904.00962},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
% Evaluation:
@inproceedings{rahimi-etal-2019-massively,
  title        = {Massively Multilingual Transfer for {NER}},
  author       = {Rahimi, Afshin  and Li, Yuan  and Cohn, Trevor},
  year         = 2019,
  month        = jul,
  booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  publisher    = {Association for Computational Linguistics},
  address      = {Florence, Italy},
  pages        = {151--164},
  url          = {https://www.aclweb.org/anthology/P19-1015}
}
@inproceedings{pan-etal-2017-cross,
  title        = {Cross-lingual Name Tagging and Linking for 282 Languages},
  author       = {Pan, Xiaoman  and Zhang, Boliang  and May, Jonathan  and Nothman, Joel  and Knight, Kevin  and Ji, Heng},
  year         = 2017,
  month        = jul,
  booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher    = {Association for Computational Linguistics},
  address      = {Vancouver, Canada},
  pages        = {1946--1958},
  doi          = {10.18653/v1/P17-1178},
  url          = {https://www.aclweb.org/anthology/P17-1178}
}
@inproceedings{pmlr-v119-hu20b,
  title        = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  author       = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  year         = 2020,
  month        = {13--18 Jul},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = 119,
  pages        = {4411--4421},
  url          = {http://proceedings.mlr.press/v119/hu20b.html},
  editor       = {Hal Daumé III and Aarti Singh},
  pdf          = {http://proceedings.mlr.press/v119/hu20b/hu20b.pdf}
}
# IndicGLUE
@inproceedings{kakwani-etal-2020-indicnlpsuite,
  title        = {{I}ndic{NLPS}uite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for {I}ndian Languages},
  author       = {Kakwani, Divyanshu  and Kunchukuttan, Anoop  and Golla, Satish  and N.C., Gokul  and Bhattacharyya, Avik  and Khapra, Mitesh M.  and Kumar, Pratyush},
  year         = 2020,
  month        = nov,
  booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {4948--4961},
  doi          = {10.18653/v1/2020.findings-emnlp.445},
  url          = {https://www.aclweb.org/anthology/2020.findings-emnlp.445}
}
@inproceedings{conneau-etal-2020-unsupervised,
  title        = {Unsupervised Cross-lingual Representation Learning at Scale},
  author       = {Conneau, Alexis  and Khandelwal, Kartikay  and Goyal, Naman  and Chaudhary, Vishrav  and Wenzek, Guillaume  and Guzm{\'a}n, Francisco  and Grave, Edouard  and Ott, Myle  and Zettlemoyer, Luke  and Stoyanov, Veselin},
  year         = 2020,
  month        = jul,
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {8440--8451},
  doi          = {10.18653/v1/2020.acl-main.747},
  url          = {https://www.aclweb.org/anthology/2020.acl-main.747}
}
@misc{Sagor_2020,
  title        = {BanglaBERT: Bengali Mask Language Model for Bengali Language Understading},
  author       = {Sagor Sarker},
  year         = 2020,
  url          = {https://github.com/sagorbrur/bangla-bert}
}
% HF ecosystem
@misc{hftokenizers2019,
  title        = {Hugging Face Tokenizers library},
  author       = {Anthony MOI and Pierric Cistac and Nicolas Patry and Evan P. Walsh and Funtowicz Morgan and Sebastian Pütz and Thomas Wolf and Sylvain Gugger and Clément Delangue and Julien Chaumond and Lysandre Debut and Patrick von Platen},
  year         = 2019,
  journal      = {GitHub repository},
  publisher    = {GitHub},
  doi          = {10.5281/zenodo.4784271},
  howpublished = {\url{https://github.com/huggingface/tokenizers}}
}
@inproceedings{butterfly_arsgd,
  title        = {An Efficient Task-Based All-Reduce for Machine Learning Applications},
  author       = {Li, Zhenyu and Davis, James and Jarvis, Stephen},
  year         = 2017,
  booktitle    = {Proceedings of the Machine Learning on HPC Environments},
  location     = {Denver, CO, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {MLHPC'17},
  doi          = {10.1145/3146347.3146350},
  isbn         = 9781450351379,
  url          = {https://doi.org/10.1145/3146347.3146350},
  articleno    = 2,
  numpages     = 8,
  keywords     = {Data-flow Frameworks, Apache Spark, Synchronous Model Training, Butterfly All-Reduce}
}
@inproceedings{wolf-etal-2020-transformers,
  title        = {Transformers: State-of-the-Art Natural Language Processing},
  author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  year         = 2020,
  month        = oct,
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {38--45},
  url          = {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}
@misc{wandb,
  title        = {Experiment Tracking with Weights and Biases},
  author       = {Biewald, Lukas},
  year         = 2020,
  url          = {https://www.wandb.com/},
  note         = {Software available from wandb.com}
}
@inproceedings{dlhub,
  title        = {DLHub: Model and Data Serving for Science},
  author       = {R. Chard and Z. Li and K. Chard and L. Ward and Y. Babuji and A. Woodard and S. Tuecke and B. Blaiszik and M. J. Franklin and I. Foster},
  year         = 2019,
  month        = {may},
  booktitle    = {2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  publisher    = {IEEE Computer Society},
  address      = {Los Alamitos, CA, USA},
  volume       = {},
  pages        = {283--292},
  doi          = {10.1109/IPDPS.2019.00038},
  issn         = {},
  url          = {https://doi.ieeecomputersociety.org/10.1109/IPDPS.2019.00038},
  keywords     = {computational modeling;metadata;adaptation models;biological system modeling;training;learning systems}
}
@inproceedings{dp_sgd,
  title        = {Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},
  author       = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  year         = 2017,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 30,
  pages        = {},
  url          = {https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
  editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@inproceedings{hole_punching,
  title        = {Peer-to-Peer Communication across Network Address Translators},
  author       = {Ford, Bryan and Srisuresh, Pyda and Kegel, Dan},
  year         = 2005,
  booktitle    = {Proceedings of the Annual Conference on USENIX Annual Technical Conference},
  location     = {Anaheim, CA},
  publisher    = {USENIX Association},
  address      = {USA},
  series       = {ATEC '05},
  pages        = 13,
  numpages     = 1
}
@inproceedings{ott2018scaling,
  title        = {Scaling Neural Machine Translation},
  author       = {Ott, Myle  and Edunov, Sergey  and Grangier, David  and Auli, Michael},
  year         = 2018,
  month        = oct,
  booktitle    = {Proceedings of the Third Conference on Machine Translation: Research Papers},
  publisher    = {Association for Computational Linguistics},
  address      = {Brussels, Belgium},
  pages        = {1--9},
  doi          = {10.18653/v1/W18-6301},
  url          = {https://www.aclweb.org/anthology/W18-6301}
}
@inproceedings{swav,
  title        = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  author       = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 33,
  pages        = {9912--9924},
  url          = {https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf},
  editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}
@misc{vissl,
  title        = {VISSL},
  author       = {Priya Goyal and Quentin Duval and Jeremy Reizenstein and Matthew Leavitt and Min Xu and Benjamin Lefaudeux and Mannat Singh and Vinicius Reis and Mathilde Caron and Piotr Bojanowski and Armand Joulin and Ishan Misra},
  year         = 2021,
  howpublished = {\url{https://github.com/facebookresearch/vissl}}
}
@inproceedings{mixed_precision,
  title        = {Mixed Precision Training},
  author       = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  year         = 2018,
  booktitle    = {International Conference on Learning Representations},
  url          = {https://openreview.net/forum?id=r1gs9JgRZ}
}
@article{foldingathome,
  title        = {Folding@home: Lessons from eight years of volunteer distributed computing},
  author       = {A. L. Beberg and D. Ensign and G. Jayachandran and S. Khaliq and V. Pande},
  year         = 2009,
  journal      = {2009 IEEE International Symposium on Parallel \& Distributed Processing},
  pages        = {1--8}
}
@article{einstein_at_home,
  title        = {Einstein@Home All-sky Search for Continuous Gravitational Waves in LIGO O2 Public Data},
  author       = {Steltner, B. and Papa, M. A. and Eggenstein, H.-B. and Allen, B. and Dergachev, V. and Prix, R. and Machenschalk, B. and Walsh, S. and Zhu, S. J. and Behnke, O. and et al.},
  year         = 2021,
  month        = {Mar},
  journal      = {The Astrophysical Journal},
  publisher    = {American Astronomical Society},
  volume       = 909,
  number       = 1,
  pages        = 79,
  doi          = {10.3847/1538-4357/abc7c9},
  issn         = {1538-4357},
  url          = {http://dx.doi.org/10.3847/1538-4357/abc7c9}
}
@article{pytorch_distributed,
  title        = {PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
  author       = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
  year         = 2020,
  month        = aug,
  journal      = {Proc. VLDB Endow.},
  publisher    = {VLDB Endowment},
  volume       = 13,
  number       = 12,
  pages        = {3005–3018},
  doi          = {10.14778/3415478.3415530},
  issn         = {2150-8097},
  url          = {https://doi.org/10.14778/3415478.3415530},
  issue_date   = {August 2020},
  numpages     = 14
}
@misc{zhu2021transfer,
  title        = {Transfer Learning in Deep Reinforcement Learning: A Survey},
  author       = {Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou},
  year         = 2021,
  eprint       = {2009.07888},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@inproceedings{7298965,
  title        = {Fully convolutional networks for semantic segmentation},
  author       = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year         = 2015,
  booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  volume       = {},
  number       = {},
  pages        = {3431--3440},
  doi          = {10.1109/CVPR.2015.7298965}
}
@inproceedings{10.1109/CVPR.2014.81,
  title        = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author       = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year         = 2014,
  booktitle    = {Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE Computer Society},
  address      = {USA},
  series       = {CVPR '14},
  pages        = {580–587},
  doi          = {10.1109/CVPR.2014.81},
  isbn         = 9781479951185,
  url          = {https://doi.org/10.1109/CVPR.2014.81},
  numpages     = 8
}
@inproceedings{pmlr-v32-donahue14,
  title        = {DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  author       = {J. Donahue and Y. Jia and Oriol Vinyals and Judy Hoffman and Ning Zhang and Eric Tzeng and Trevor Darrell},
  year         = 2014,
  booktitle    = {ICML}
}
@misc{johnson2016perceptual,
  title        = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  author       = {Justin Johnson and Alexandre Alahi and Li Fei-Fei},
  year         = 2016,
  eprint       = {1603.08155},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@misc{honda2019smiles,
  title        = {SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery},
  author       = {Shion Honda and Shoi Shi and Hiroki R. Ueda},
  year         = 2019,
  eprint       = {1911.04738},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@article{Lu2020.09.04.283929,
  title        = {Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization},
  author       = {Lu, Amy X. and Zhang, Haoran and Ghassemi, Marzyeh and Moses, Alan},
  year         = 2020,
  journal      = {bioRxiv},
  publisher    = {Cold Spring Harbor Laboratory},
  doi          = {10.1101/2020.09.04.283929},
  url          = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929},
  elocation-id = {2020.09.04.283929},
  eprint       = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929.full.pdf}
}
@inproceedings{NEURIPS2020_92d1e1eb,
  title        = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author       = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  year         = 2020,
  booktitle    = {Advances in Neural Information Processing Systems},
  publisher    = {Curran Associates, Inc.},
  volume       = 33,
  pages        = {12449--12460},
  url          = {https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
  editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}
@misc{tfhub,
  author       = {{TensorFlow Hub}},
  title        = {{TensorFlow} {H}ub},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://www.tensorflow.org/hub}}
}
@article{meshtensorflow,
  title        = {Mesh-TensorFlow: Deep Learning for Supercomputers},
  author       = {Noam Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and HyoukJoong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake A. Hechtman},
  year         = 2018,
  journal      = {CoRR},
  volume       = {abs/1811.02084},
  url          = {http://arxiv.org/abs/1811.02084},
  eprinttype   = {arXiv},
  eprint       = {1811.02084},
  timestamp    = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-02084.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{torchhub,
  author       = {{PyTorch Hub}},
  title        = {{PyTorch} {H}ub},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://pytorch.org/hub/}}
}
@misc{hfhub,
  title        = {{Hugging Face} {H}ub},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://huggingface.co/models}}
}
@inproceedings{Tapparello2016VolunteerCO,
  title        = {Volunteer Computing on Mobile Devices: State of the Art and Future Research Directions},
  author       = {C. Tapparello and Colin Funai and Shurouq Hijazi and Abner Aquino and Bora Karaoglu and H. Ba and J. Shi and W. Heinzelman},
  year         = 2016,
  booktitle    = {Enabling Real-Time Mobile Cloud Computing through Emerging Technologies},
  pages        = {153--181}
}
@misc{shi2018performance,
  title        = {Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs},
  author       = {Shaohuai Shi and Qiang Wang and Xiaowen Chu},
  year         = 2018,
  eprint       = {1711.05979},
  archiveprefix = {arXiv},
  primaryclass = {cs.DC}
}
@misc{sergeev2018horovod,
  title        = {Horovod: fast and easy distributed deep learning in TensorFlow},
  author       = {Alexander Sergeev and Mike Del Balso},
  year         = 2018,
  eprint       = {1802.05799},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@misc{google_stadia,
  title        = {{Google Stadia} data usage},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://support.google.com/stadia/answer/9607891}}
}
@misc{netflix,
  title        = {{N}etflix data usage},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://help.netflix.com/en/node/87}}
}
@inproceedings{liu-etal-2020-understanding,
  title        = {Understanding the Difficulty of Training Transformers},
  author       = {Liu, Liyuan  and Liu, Xiaodong  and Gao, Jianfeng  and Chen, Weizhu  and Han, Jiawei},
  year         = 2020,
  month        = nov,
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  publisher    = {Association for Computational Linguistics},
  address      = {Online},
  pages        = {5747--5763},
  doi          = {10.18653/v1/2020.emnlp-main.463},
  url          = {https://www.aclweb.org/anthology/2020.emnlp-main.463}
}
@inproceedings{projectadam,
  title        = {Project Adam: Building an Efficient and Scalable Deep Learning Training System},
  author       = {Trishul Chilimbi and Yutaka Suzue and Johnson Apacible and Karthik Kalyanaraman},
  year         = 2014,
  month        = oct,
  booktitle    = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  publisher    = {{USENIX} Association},
  address      = {Broomfield, CO},
  pages        = {571--582},
  isbn         = {978-1-931971-16-4},
  url          = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi}
}
@misc{aji2019making,
  title        = {Making Asynchronous Stochastic Gradient Descent Work for Transformers},
  author       = {Alham Fikri Aji and Kenneth Heafield},
  year         = 2019,
  eprint       = {1906.03496},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@misc{jain2020indictransformers,
  title        = {Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages},
  author       = {Kushal Jain and Adwait Deshpande and Kumar Shridhar and Felix Laumann and Ayushman Dash},
  year         = 2020,
  eprint       = {2011.02323},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@misc{libp2p,
  title        = {libp2p},
  note         = {Accessed: 2021-10-04},
  howpublished = {\url{https://libp2p.io/}}
}
@misc{tensorflow2015-whitepaper,
  title        = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  author       = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  year         = 2015,
  url          = {https://www.tensorflow.org/},
  note         = {Software available from tensorflow.org}
}
@article{loshchilov2017decoupled,
  title        = {Decoupled weight decay regularization},
  author       = {Loshchilov, Ilya and Hutter, Frank},
  year         = 2017,
  journal      = {arXiv preprint arXiv:1711.05101}
}
@inproceedings{goodfellow2013maxout,
  title        = {Maxout Networks},
  author       = {Ian J. Goodfellow and David Warde{-}Farley and Mehdi Mirza and Aaron C. Courville and Yoshua Bengio},
  year         = 2013,
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013},
  publisher    = {JMLR.org},
  series       = {{JMLR} Workshop and Conference Proceedings},
  volume       = 28,
  pages        = {1319--1327},
  url          = {http://proceedings.mlr.press/v28/goodfellow13.html},
  timestamp    = {Wed, 29 May 2019 08:41:45 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/GoodfellowWMCB13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gokaslan2019openwebtext,
  title        = {Openwebtext corpus},
  author       = {Gokaslan, Aaron and Cohen, Vanya},
  year         = 2019,
  journal      = {urlhttp://Skylion007. github. io/OpenWebTextCorpus}
}
@inproceedings{tang2021adam1bit,
  title        = {1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed},
  author       = {Hanlin Tang and Shaoduo Gan and Ammar Ahmad Awan and Samyam Rajbhandari and Conglong Li and Xiangru Lian and Ji Liu and Ce Zhang and Yuxiong He},
  year         = 2021,
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event},
  publisher    = {{PMLR}},
  series       = {Proceedings of Machine Learning Research},
  volume       = 139,
  pages        = {10118--10129},
  url          = {http://proceedings.mlr.press/v139/tang21a.html},
  editor       = {Marina Meila and Tong Zhang},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/TangGARLLLZH21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{li2021lamb1bit,
  title        = {1-bit {LAMB:} Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed},
  author       = {Conglong Li and Ammar Ahmad Awan and Hanlin Tang and Samyam Rajbhandari and Yuxiong He},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2104.06069},
  url          = {https://arxiv.org/abs/2104.06069},
  eprinttype   = {arXiv},
  eprint       = {2104.06069},
  timestamp    = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-06069.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{vogels2019powersgd,
  title        = {PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization},
  author       = {Thijs Vogels and Sai Praneeth Karimireddy and Martin Jaggi},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  pages        = {14236--14245},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/d9fbed9da256e344c1fa46bb46c34c5f-Abstract.html},
  editor       = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  timestamp    = {Thu, 21 Jan 2021 15:15:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VogelsKJ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{krizhevsky2014oneweirdtrick,
  title        = {One weird trick for parallelizing convolutional neural networks},
  author       = {Alex Krizhevsky},
  year         = 2014,
  journal      = {CoRR},
  volume       = {abs/1404.5997},
  url          = {http://arxiv.org/abs/1404.5997},
  eprinttype   = {arXiv},
  eprint       = {1404.5997},
  timestamp    = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Krizhevsky14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang2018training8bit,
  title        = {Training Deep Neural Networks with 8-bit Floating Point Numbers},
  author       = {Naigang Wang and Jungwook Choi and Daniel Brand and Chia{-}Yu Chen and Kailash Gopalakrishnan},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {7686--7695},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/335d3d1cd7ef05ec77714a215134914c-Abstract.html},
  editor       = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/WangCBCG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sun2019hybrid8bit,
  title        = {Hybrid 8-bit Floating Point {(HFP8)} Training and Inference for Deep Neural Networks},
  author       = {Xiao Sun and Jungwook Choi and Chia{-}Yu Chen and Naigang Wang and Swagath Venkataramani and Vijayalakshmi Srinivasan and Xiaodong Cui and Wei Zhang and Kailash Gopalakrishnan},
  year         = 2019,
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  pages        = {4901--4910},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html},
  editor       = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  timestamp    = {Thu, 21 Jan 2021 15:15:19 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/SunCCWVSCZG19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{cambier2020shiftsqueeze,
  title        = {Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks},
  author       = {L{\'{e}}opold Cambier and Anahita Bhiwandiwalla and Ting Gong and Oguz H. Elibol and Mehran Nekuii and Hanlin Tang},
  year         = 2020,
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=Bkxe2AVtPS},
  timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/CambierBGENT20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{drumond2018hybridblock,
  title        = {Training DNNs with Hybrid Block Floating Point},
  author       = {Mario Drumond and Tao Lin and Martin Jaggi and Babak Falsafi},
  year         = 2018,
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {451--461},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.html},
  editor       = {Samy Bengio and Hanna M. Wallach and Hugo Larochelle and Kristen Grauman and Nicol{\`{o}} Cesa{-}Bianchi and Roman Garnett},
  timestamp    = {Tue, 01 Jun 2021 10:12:08 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/DrumondLJF18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{qin2020survey1bit,
  title        = {Binary Neural Networks: {A} Survey},
  author       = {Haotong Qin and Ruihao Gong and Xianglong Liu and Xiao Bai and Jingkuan Song and Nicu Sebe},
  year         = 2020,
  journal      = {CoRR},
  volume       = {abs/2004.03333},
  url          = {https://arxiv.org/abs/2004.03333},
  eprinttype   = {arXiv},
  eprint       = {2004.03333},
  timestamp    = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-03333.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2019bit4,
  title        = {Fully Quantized Network for Object Detection},
  author       = {Rundong Li and Yan Wang and Feng Liang and Hongwei Qin and Junjie Yan and Rui Fan},
  year         = 2019,
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019},
  publisher    = {Computer Vision Foundation / {IEEE}},
  pages        = {2810--2819},
  doi          = {10.1109/CVPR.2019.00292},
  url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Li\_Fully\_Quantized\_Network\_for\_Object\_Detection\_CVPR\_2019\_paper.html},
  timestamp    = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/LiWLQYF19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gong2019softquant,
  title        = {Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks},
  author       = {Ruihao Gong and Xianglong Liu and Shenghu Jiang and Tianxiang Li and Peng Hu and Jiazhen Lin and Fengwei Yu and Junjie Yan},
  year         = 2019,
  booktitle    = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019},
  publisher    = {{IEEE}},
  pages        = {4851--4860},
  doi          = {10.1109/ICCV.2019.00495},
  url          = {https://doi.org/10.1109/ICCV.2019.00495},
  timestamp    = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl       = {https://dblp.org/rec/conf/iccv/GongLJLHLYY19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhu2017ternary,
  title        = {Trained Ternary Quantization},
  author       = {Chenzhuo Zhu and Song Han and Huizi Mao and William J. Dally},
  year         = 2017,
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=S1\_pAu9xl},
  timestamp    = {Fri, 20 Nov 2020 16:16:07 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhuHMD17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{choi2019bit2,
  title        = {Accurate and Efficient 2-bit Quantized Neural Networks},
  author       = {Jungwook Choi and Swagath Venkataramani and Vijayalakshmi Srinivasan and Kailash Gopalakrishnan and Zhuo Wang and Pierce Chuang},
  year         = 2019,
  booktitle    = {Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019},
  publisher    = {mlsys.org},
  url          = {https://proceedings.mlsys.org/book/268.pdf},
  editor       = {Ameet Talwalkar and Virginia Smith and Matei Zaharia},
  timestamp    = {Thu, 18 Jun 2020 15:48:01 +0200},
  biburl       = {https://dblp.org/rec/conf/mlsys/ChoiVSGWC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Rastegari2016xnor,
  title        = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
  author       = {Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
  year         = 2016,
  booktitle    = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part {IV}},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  volume       = 9908,
  pages        = {525--542},
  doi          = {10.1007/978-3-319-46493-0\_32},
  url          = {https://doi.org/10.1007/978-3-319-46493-0\_32},
  editor       = {Bastian Leibe and Jiri Matas and Nicu Sebe and Max Welling},
  timestamp    = {Wed, 25 Sep 2019 18:11:12 +0200},
  biburl       = {https://dblp.org/rec/conf/eccv/RastegariORF16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{courbariaux2015binaryconnect,
  title        = {BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
  author       = {Matthieu Courbariaux and Yoshua Bengio and Jean{-}Pierre David},
  year         = 2015,
  booktitle    = {Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  pages        = {3123--3131},
  url          = {https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html},
  editor       = {Corinna Cortes and Neil D. Lawrence and Daniel D. Lee and Masashi Sugiyama and Roman Garnett},
  timestamp    = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/CourbariauxBD15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{courbariaux2016bit1,
  title        = {BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
  author       = {Matthieu Courbariaux and Yoshua Bengio},
  year         = 2016,
  journal      = {CoRR},
  volume       = {abs/1602.02830},
  url          = {http://arxiv.org/abs/1602.02830},
  eprinttype   = {arXiv},
  eprint       = {1602.02830},
  timestamp    = {Mon, 13 Aug 2018 16:46:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/CourbariauxB16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{courbariaux2014training,
  title        = {Training deep neural networks with low precision multiplications},
  author       = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  year         = 2014,
  journal      = {arXiv preprint arXiv:1412.7024}
}
@article{mellempudi2019bit8,
  title        = {Mixed Precision Training With 8-bit Floating Point},
  author       = {Naveen Mellempudi and Sudarshan Srinivasan and Dipankar Das and Bharat Kaul},
  year         = 2019,
  journal      = {CoRR},
  volume       = {abs/1905.12334},
  url          = {http://arxiv.org/abs/1905.12334},
  eprinttype   = {arXiv},
  eprint       = {1905.12334},
  timestamp    = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-12334.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{han2015deepcompression,
  title        = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author       = {Song Han and Huizi Mao and William J. Dally},
  year         = 2016,
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  url          = {http://arxiv.org/abs/1510.00149},
  editor       = {Yoshua Bengio and Yann LeCun},
  timestamp    = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/HanMD15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ramesh2021dalle,
  title        = {Zero-Shot Text-to-Image Generation},
  author       = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  year         = 2021,
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event},
  publisher    = {{PMLR}},
  series       = {Proceedings of Machine Learning Research},
  volume       = 139,
  pages        = {8821--8831},
  url          = {http://proceedings.mlr.press/v139/ramesh21a.html},
  editor       = {Marina Meila and Tong Zhang},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/RameshPGGVRCS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{rag,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{base,
  author    = {Mike Lewis and
               Shruti Bhosale and
               Tim Dettmers and
               Naman Goyal and
               Luke Zettlemoyer},
  title     = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  journal   = {CoRR},
  volume    = {abs/2103.16716},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.16716},
  eprinttype = {arXiv},
  eprint    = {2103.16716},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-16716.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{2205.05638,
  doi = {10.48550/ARXIV.2205.05638},
  
  url = {https://arxiv.org/abs/2205.05638},
  
  author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{shazeer2018adafactor,
  title        = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author       = {Noam Shazeer and Mitchell Stern},
  year         = 2018,
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
  publisher    = {{PMLR}},
  series       = {Proceedings of Machine Learning Research},
  volume       = 80,
  pages        = {4603--4611},
  url          = {http://proceedings.mlr.press/v80/shazeer18a.html},
  editor       = {Jennifer G. Dy and Andreas Krause},
  timestamp    = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/ShazeerS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{fan2020layerdrop,
  title        = {Reducing Transformer Depth on Demand with Structured Dropout},
  author       = {Angela Fan and Edouard Grave and Armand Joulin},
  year         = 2020,
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=SylO2yStDr},
  timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/FanGJ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{stock2021quantnoise,
  title        = {Training with Quantization Noise for Extreme Model Compression},
  author       = {Pierre Stock and Angela Fan and Benjamin Graham and Edouard Grave and R{\'{e}}mi Gribonval and Herv{\'{e}} J{\'{e}}gou and Armand Joulin},
  year         = 2021,
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=dV19Yyi1fS3},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/StockFGGGJJ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{ba2016layernorm,
  title        = {Layer Normalization},
  author       = {Lei Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  year         = 2016,
  journal      = {CoRR},
  volume       = {abs/1607.06450},
  url          = {http://arxiv.org/abs/1607.06450},
  eprinttype   = {arXiv},
  eprint       = {1607.06450},
  timestamp    = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{huffman1952method,
  title        = {A method for the construction of minimum-redundancy codes},
  author       = {Huffman, David A},
  year         = 1952,
  journal      = {Proceedings of the IRE},
  publisher    = {IEEE},
  volume       = 40,
  number       = 9,
  pages        = {1098--1101}
}
@misc{deberta,
  title        = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author       = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  year         = 2020,
  eprint       = {2006.03654},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@article{curriculum_minja,
  title        = {Curriculum Learning: {A} Regularization Method for Efficient and Stable Billion-Scale {GPT} Model Pre-Training},
  author       = {Conglong Li and Minjia Zhang and Yuxiong He},
  year         = 2021,
  journal      = {CoRR},
  volume       = {abs/2108.06084},
  url          = {https://arxiv.org/abs/2108.06084},
  eprinttype   = {arXiv},
  eprint       = {2108.06084},
  timestamp    = {Wed, 18 Aug 2021 19:45:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-06084.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{interleaved_round_robin,
  title        = {Interleaved Weighted Round-Robin: A Network Calculus Analysis},
  author       = {Tabatabaee, Seyed Mohammadhossein and Le Boudec, Jean-Yves and Boyer, Marc},
  year         = 2020,
  booktitle    = {2020 32nd International Teletraffic Congress (ITC 32)},
  volume       = {},
  number       = {},
  pages        = {64--72},
  doi          = {10.1109/ITC3249928.2020.00016}
}
@inproceedings{gpt,
  title        = {Improving Language Understanding by Generative Pre-Training},
  author       = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  year         = 2018,
  url          = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}
@software{gptneo,
  title        = {{GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}},
  author       = {Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and Biderman, Stella},
  year         = 2021,
  month        = mar,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715},
  note         = {{If you use this software, please cite it using these metadata.}},
  version      = {1.0}
}
@misc{gptj,
  title        = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  author       = {Wang, Ben and Komatsuzaki, Aran},
  year         = 2021,
  month        = May,
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}}
}
@misc{gao2020pile,
  title        = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author       = {Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
  year         = 2020,
  eprint       = {2101.00027},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@misc{hivemind,
  title        = {{H}ivemind: a {L}ibrary for {D}ecentralized {D}eep {L}earning},
  author       = {Team Learning@home},
  year         = 2020,
  howpublished = {\url{https://github.com/learning-at-home/hivemind}}
}
@misc{FairScale2021,
  title        = {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},
  author       = {Mandeep Baines and Shruti Bhosale and Vittorio Caggiano and Naman Goyal and Siddharth Goyal and Myle Ott and Benjamin Lefaudeux and Vitaliy Liptchinsky and Mike Rabbat and Sam Sheiffer and Anjali Sridhar and Min Xu},
  year         = 2021,
  howpublished = {\url{https://github.com/facebookresearch/fairscale}}
}
@misc{su2021roformer,
  title        = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author       = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  year         = 2021,
  eprint       = {2104.09864},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@article{iwrr,
  title        = {Weighted round-robin cell multiplexing in a general-purpose ATM switch chip},
  author       = {Katevenis, M. and Sidiropoulos, S. and Courcoubetis, C.},
  year         = 1991,
  journal      = {IEEE Journal on Selected Areas in Communications},
  volume       = 9,
  number       = 8,
  pages        = {1265--1279},
  doi          = {10.1109/49.105173}
}
@article{model_parallelism_survey1,
  title        = {Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis},
  author       = {Ben-Nun, Tal and Hoefler, Torsten},
  year         = 2019,
  month        = {aug},
  journal      = {ACM Comput. Surv.},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = 52,
  number       = 4,
  doi          = {10.1145/3320060},
  issn         = {0360-0300},
  url          = {https://doi.org/10.1145/3320060},
  issue_date   = {September 2019},
  abstract     = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
  articleno    = 65,
  numpages     = 43,
  keywords     = {Deep learning, parallel algorithms, distributed computing}
}
@misc{model_parallelism_survey2,
  title        = {Communication-Efficient Distributed Deep Learning: A Comprehensive Survey},
  author       = {Zhenheng Tang and Shaohuai Shi and Xiaowen Chu and Wei Wang and Bo Li},
  year         = 2020,
  eprint       = {2003.06307},
  archiveprefix = {arXiv},
  primaryclass = {cs.DC}
}
@inproceedings{baevski2019adaptiveinputs,
  title        = {Adaptive Input Representations for Neural Language Modeling},
  author       = {Alexei Baevski and Michael Auli},
  year         = 2019,
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=ByxZX20qFQ},
  timestamp    = {Thu, 25 Jul 2019 14:26:00 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/BaevskiA19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{lin2021survey,
      title={A Survey of Transformers}, 
      author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
      year={2021},
      eprint={2106.04554},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{adapterhub,
  title={Adapterhub: A framework for adapting transformers},
  author={Pfeiffer, Jonas and R{\"u}ckl{\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2007.07779},
  year={2020}
}

@inproceedings{lee2022deduplicating,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8424--8445},
  year={2022}
}

@article{chinchilla,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@inproceedings{ptuning,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.8",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
}

@misc{gptj,
  title        = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  author       = {Wang, Ben and Komatsuzaki, Aran},
  year         = 2021,
  month        = May,
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}}
}

@misc{raffel2021call,
  title        = {A Call to Build Models Like We Build Open-Source Software},
  author       = {Colin Raffel},
  year         = 2021,
  url = {https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html}
}

@misc{zeng2020glm,
  title        = {{GLM-130B}: An Open Bilingual Pre-Trained Model},
  author       = {Aohan Zeng and Xiao Liu and Zhengxiao Du and Ming Ding and Qinkai Zheng and Hanyu Lai and Zihan Wang and Zhuoyi Yang and Jifan Yu and Xiaohan Zhang and Wendi Zheng and Xiao Xia and Yifan Xu and Weng Lam Tam and Yuxiao Dong and Zixuan Ma and Jiaao He and Zhenbo Sun and Jidong Zhai and Wenguang Chen and Guoyang Zeng and Xu Han and Weilin Zhao and Zhiyuan Liu and Yufei Xue and Shan Wang and Jiecai Shan and Haohan Jiang and Zhengang Guo and Peng Zhang and Jie Tang},
  year         = 2022,
  url = {http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/}
}

@inproceedings{guo2021parameter,
  title={Parameter-Efficient Transfer Learning with Diff Pruning},
  author={Guo, Demi and Rush, Alexander M and Kim, Yoon},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}


@article{sung2021training,
  title={Training neural networks with fixed sparse masks},
  author={Sung, Yi-Lin and Nair, Varun and Raffel, Colin},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{rajbhandari2021zero,
  title={Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2021}
}
