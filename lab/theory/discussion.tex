\section{Discussion and future work}\label{sect:discussion}\label{sect:incentives}

\paragraph{Incentives for peers to contribute.}
In \textsc{Petals}, peers using the client are not required to run a server. Naturally, this may lead to an imbalance between supply (peers who dedicate GPUs to serve model layers) and demand (peers using the servers to perform inference or fine-tuning for their own needs) in the network.
One way to encourage users to serve model layers would be to introduce a system of \textit{incentives}: peers running servers would earn special \textit{points}, which can be spent on high-priority inference and fine-tuning or exchanged for other rewards.
% This system may be implemented using a centralized ``accounting'' server or in a decentralized way, using one of the common digital ledgers \cite{ethereum,near}.
We do not include such an incentive system in this demonstration to keep it focused on the technical aspects; however, it may be developed and deployed once an active user base develops.
% We would develop such an incentive system once an active user base develops. We do not implement it for this demonstration because without users the incentive system would not have any benefit and would make the usage of our system more cumbersome at its current stage.

\paragraph{Security.}
We assume that servers in our system are run by many independent parties. In practice, some of them may turn out to be faulty and return incorrect outputs instead of the actual results of forward and backward passes. This may happen due to a malicious intent to influence other people's outputs or, when rewards are introduced (as described above), to earn a reward for serving layers without actually performing the calculations.

A possible way to address these issues would be to use an economically motivated approach.
Some servers may vouch for the correctness of their outputs (e.g., in exchange for increased inference price) by depositing a certain number of points as a pledge. Then, for each request, they announce a cryptographic hash of the input and output tensors, so anyone having the inputs can check whether the outputs are correct.

If someone finds a mismatch confirmed by a trusted third party, they can claim the server's pledge as a reward. In practice, it may be a client who suspects that they received wrong outputs or a ``bounty hunter'' sending requests to different servers in the hope of catching errors.
While this approach still leaves a chance of receiving wrong outputs, it makes cheating costly and creates an incentive to quickly expose the malicious servers.

\paragraph{Privacy.} A key limitation of our approach is that peers serving the first layers of the model can use their inputs to recover input tokens. Thus, \textit{clients working with sensitive data should only use the servers hosted by trusted institutions that are allowed to process this data.}
This limitation may be addressed in future work using secure multi-party computing~\cite{evans2018pragmatic} or privacy-preserving hardware~\cite{nvidia-privacy}.

% [[Longer version]]:

% A server $S$ willing to do that should deposit $P$ points to a trusted third-party $T$ as a pledge that $S$ will return only correct outputs.
% Then, for each incoming request, $S$ should announce a cryptographic hash of the input and output tensors, so anyone who has the input tensors would be able to check whether the outputs are correct.

% If some peer $H$ manages to find an input where $S$ has cheated, it presents this input to $T$. If $T$ confirms the fact of cheating, it announces that $S$ is a cheater to all clients. The deposit is taken away from $S$ and divided between $H$ and $T$ (since both of them spent their compute to confirm the fact of cheating). In practice, the role of $H$ may be performed by either a client who suspect that they received wrong outputs, or a ``bounty hunter'' who sends various requests to different servers hoping to catch a cheater, occasionally earning bounties and keeping cheaters out of the system.

% Otherwise, if no cases of $S$ cheating were confirmed for $t$ hours after $S$ served its last request, $S$ is allowed to redeem its deposit along with earnings for all processed requests.

% While this approach still leaves the probability that a server may return wrong outputs, it makes cheating unprofitable on average and creates incentive to quickly detect and ban the cheating servers who vouched for their results.

\paragraph{Making changes to the main model.}
As discussed in Section~\ref{sect:design_training}, distributed parameter-efficient fine-tuning makes it easy for users to apply the base model to new tasks.
In Section~\ref{sect:design_ecosystem}, we also described how these updates can be easily shared and reused by others.
This capability provides a meaningful step towards \textit{collaborative} improvement of machine learning models~\cite{raffel2021call}:
as more and more users train the base model, it will effectively become more capable over time.

% The possibility of collaborative model development opens up research questions that have seen limited work.
% For example, consider the setting wh  ere multiple users have fine-tuned the model on related tasks.
% We might hope to improve the performance on each of the tasks by combining what was learned on the other tasks (which could be considered a form of post-hoc intermediate-task training, similarly to~\citealp{stilts}).
% Recent preliminary work~\cite{matena2021merging} has shown that it is possible to ``merge'' the results of different fine-tuning runs to improve performance on a target task.

Furthermore, we might expect the model parameters that perform best on a specific task to change over time.
Similarly to version control systems for code, it would be useful to track versions of fine-tuned model parameters as they change.
A system for rapidly testing the performance of a set of parameters on ``living benchmarks''~\cite{dynabench,gehrmann2022gemv2,eval-harness} would be valuable to ensure that subsequent versions improve the desired capabilities.
% Finally, ``living benchmarks'' have a potential of synergy with continuously updated models: as the performance of a model (or its adapter) saturates on a given task, we might include adversarial examples or shift our focus to other aspects of generated text, using distributed inference for rapid prototyping.

Apart from adaptation to new tasks, it would also be useful to eventually update the main model.
% with increasing efforts in dataset collection~\cite{gao2020pile, lee2022deduplicating} and better understanding of pretraining itself~\cite{chinchilla}, we might expect that retraining even the same architecture can improve its general capabilities.
Ideally, such updates could be tracked in a principled way.
Users of \textsc{Petals} could specify the versions of the model they want to use, and servers could indicate which versions they support.
Introducing a newer version of the model then reduces to adding a new group of layers, which then naturally supersedes older parameters based on the approach from Section~\ref{sect:networking}.
Similarly, fine-tuned model adapters could be annotated with tags denoting the model version they are applicable for.
Such fine-grained versioning of models is currently uncommon but would be straightforward to add to \textsc{Petals}.
% , assuming that updates of the whole model happen rarely enough for the servers to migrate successfully.

\section{Conclusion}

This paper introduces \textsc{Petals}, a system for efficient collaborative inference and fine-tuning of large language models. We offer a user-friendly generation interface and a flexible API to access models served over the Internet. We use 8-bit compression that reduces the resource requirements to run very large models. In addition, we develop algorithms for reliable routing and load balancing.

Since \textsc{Petals} is open-source, we would like it to evolve based on the community's feedback, incorporating relevant research advances and adding support for features in demand.
With the release of this system, we hope to broaden access to large language models and pave the road to applications, studies or research questions that were previously not possible or simply too expensive.


% We discuss future problems such as privacy, security, incentive structures, and further adaptability of models. All-in-all, \textsc{Petals} promises wide accessibility to very large models on consumer hardware through distributed collaboration.