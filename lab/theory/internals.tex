\section{Internal structure and optimizations}\label{sect:internals}


One of the primary considerations for distributed inference is its performance. It can be broken down into three main aspects: computation speed (5-year-old gaming GPU vs. new data center GPU), communication delay due to distance between nodes (intercontinental vs. local), and communication delay due to bandwidth (10 Mbit/s vs. 10 Gbit/s).

In terms of raw FLOPs, even consumer-grade GPUs like GeForce RTX 3070 could run a complete inference step of BLOOM-176B in less than a second~\cite{ga102-datasheet}. However, the GPU memory can only hold a small fraction of model layers: running na\"ively would require 44 RTX 3070 GPUs and 44 communication rounds. To make this more efficient, we use quantization to store more parameters per GPU, reducing the number of consecutive devices and communication rounds (Section~\ref{sect:inside_gpu}). On top of that, each client prioritizes nearby servers to make communication rounds faster (Section~\ref{sect:networking}).

% partially moved to 3.2
%While GPU performance differs by roughly 3-5x at most, communication time can differ by many orders of magnitude. %(local Gbit communication vs global MBit communication).
%Latency can even be unbounded in case of node failures. As such, optimizations related to latency are the most critical components for a practical distributed inference system â€” a single high latency communication can lead to poor inference runtime performance.

% this paragraph was moved to 3.2
% We can optimize latency through  optimizations, that is, compressing weights and hidden states, and, secondly, through graph optimizations, that is optimize the communication between users so that the total communication cost for an inference pass is on average minimal.

\subsection{Large model inference on consumer GPUs}\label{sect:inside_gpu}

% YOZH: paraphrased this sentence, moved some of the motivation to 3.0
% orginal: One of our main goals is democratization of large pretrained models. As such, our distributed inference method is designed to work well on
We assume that each server has at least 16 GB of CPU RAM, 8 GB of GPU memory. From this assumption, one of the primary considerations is to reduce the model memory footprint, so that each device can hold more Transformer blocks.% [MOVED TO Section 3.0], thus less communication between users has to be done to process all transformers blocks to complete a forward/backward pass.

For example, BLOOM has 176B parameters, which takes 352 GB of GPU memory in 16-bit precision. Thus, in the worst case, the model is distributed among 352 GB / 8 GB (per server) = 44 nodes. We can reduce both frequency and amount of data transfer in two ways.
First, we can achieve this by compressing the hidden states exchanged between nodes. Second, we can compress the weights to 8-bit precision, reducing the number of nodes required to hold all layers. For BLOOM, this changes the number of required nodes from 44 to 22, which reduces latency in half and decreases the probability of a failure.

\paragraph{Compressing communication buffers.} To send less data between subsequent pipeline stages, we use dynamic blockwise quantization \citep{dettmers2022optimizers}. We apply it to the hidden states before pipeline-parallel communication, as done in \citet{ryabinin2021swarm}. Dynamic blockwise quantization halves the bandwidth requirements without any noticeable effect on generation quality.

\paragraph{Compressing model weights.} We use 8-bit mixed matrix decomposition for matrix multiplication to quantize the weights to 8-bit precision and reduce the memory footprint compared to 16-bit weights, as suggested in \cite{dettmers2022llm}. This decomposition separates hidden states and weights into two portions: about 0.1\% of 16-bit outlier and 99.9\% of 8-bit regular values, which roughly halves the memory footprint.

% \begin{table}[tb]
% \centering
% \caption{Zero-shot accuracy for OPT-175B and BLOOM-176B with 8-bit and 16-bit weights.\nocite{eval-harness}}
% \vspace{-8px}
% \label{tbl:resutls8bit}
% % \small
% % \setlength{\tabcolsep}{2pt}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lcccccc}\toprule
% \bf Model    &\bf Bits &\bf HellaSwag &\bf PIQA & \bf LAMBADA &\bf WinoGrande &\bf Avg  \\\midrule
% \multirow{2}{*}{OPT-175B} & 16   & 78      & 81 & 75    & 72       & 77 \\
%  & 8    & 78      & 81 & 75    & 72       & 77 \\\midrule
% \multirow{2}{*}{BLOOM}    & 16   & 73      & 79 & 67    & 70       & 73 \\
%    & 8    & 73      & 79 & 68    & 70       & 73 \\\bottomrule
% \end{tabular}%
% }
% \label{tab:quality}
% \vspace{-4pt}

% \end{table}

\begin{table}[tb]
\centering
\caption{Zero-shot accuracy for OPT-175B and BLOOM-176B with 8-bit and 16-bit weights.\nocite{eval-harness}}
\vspace{-5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccc}\toprule
\textbf{Model}            & \textbf{Bits} & \textbf{HellaSwag} & \textbf{LAMBADA} & \textbf{WinoGrande} & \textbf{Avg}             \\\midrule
\multirow{2}{*}{OPT-175B} & 16            & 78.5               & 74.7             & 72.6                & 75.3                     \\
                          & 8             & 78.5               & 74.6             & 71.7                & \multicolumn{1}{r}{74.9} \\\midrule
\multirow{2}{*}{BLOOM}    & 16            & 73.0               & 67.2             & 70.1                & 70.1                     \\
                          & 8             & 72.8               & 68.1             & 70.1                & 70.3                    \\\bottomrule
\end{tabular}}
\label{tab:quality}
% \vspace{-5pt}
\end{table}

As shown in Table~\ref{tab:quality}, this method has little effect on LLM quality for major benchmarks.
In terms of inference time, Table~\ref{tab:throughput} demonstrates that quantization has about $5\%$ of overhead with batch size 1 (20 tokens), but becomes negligible for larger batches.

% TODO:
% \begin{itemize}
%     \item quality benchmarks v.s. original model (Tim: I need to write some CUDA kernels for the final data; initial tests indicate no ppl degradation on OPT-66B)
%     \item performance (aka speed) benchmarks v.s. original model (locally) (Younes has these completed for BLOOM)
%     \item any extra optimizations?
%     \item how to convert your own model (we have HF integration, if distribute inference is build on the HF pretrained models this should work for any model)
% \end{itemize}



\begin{table}[tb]
% \vspace{-8px}
\centering
\caption{Generation throughput (tokens/s) for BLOOM-176B on 8 A100 GPUs with 8-bit and 16-bit weights.}
\vspace{-5pt}
\label{tbl:memory_footprint}
\resizebox{0.58\linewidth}{!}{%
\begin{tabular}{lccc}\toprule
\multirow{2}{*}{\bf Weights}& \multicolumn{3}{c}{\bf Batch size} \\\cmidrule{2-4} & \bf 1 & \bf 8 &\bf 32  \\\toprule
16-bit       & 4.18 & 31.3  & 100.6  \\
8-bit        & 3.95 & 29.4  & 95.8\\\bottomrule
\end{tabular}}
\vspace{-10pt}
\label{tab:throughput}
\end{table}




\subsection{Collaborating over the Internet}\label{sect:networking}

Another important challenge is to provide \textit{reliable} inference and training despite nodes joining, leaving or failing at any time. To address this, \textsc{Petals} uses the \texttt{hivemind} library~\cite{hivemind} for decentralized training and custom fault-tolerant protocols for servers and clients.

% \begin{itemize}
%     \item Design constraints? (limited network bandwidth,  heterogeneous devices, hardware and network failures)
%     \item activation quantization
%     \item user doesn't want manual tuning, manual error handling, etc
%     \item latency-aware activation routing
%     \item servers determine which blocks are in most demand
%     \item servers are as stateless as possible, client stores everything needed to recover from server failure
%     \item how to convert your own model
% \end{itemize}

\paragraph{Server load balancing.} First, we ensure that servers are distributed evenly among Transformer blocks. Formally, servers maximize the total model throughput by choosing the blocks with the worst throughput and eliminating potential bottlenecks.

Each server periodically announces its active blocks to a distributed hash table~\cite{kademlia}. When a new server joins, it uses this information to identify an interval of blocks that contains most blocks with the worst throughput. This interval is always contiguous, since splitting it would harm the inference latency. Once the server has selected its layers, it measures its own throughput (both network and compute) and announces it to the distributed hash table. %(the minimum of its network and compute throughputs).

Since peers may leave or fail at any time, all nodes periodically check if launching a rebalancing procedure would significantly improve the overall throughput. If it is the case, they switch layers until the throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave the system, this procedure quickly redistributes the remaining resources to close the emerged gaps.

% \textbf{Efficient network communication.} During inference or training, both clients and servers send high dimensional intermediate activations or gradients through the Internet. Therefore, high network latency and limited bandwidth can significantly reduce throughput of the overall system. 

% By default, peers communicate via half-precision tensors. However, we also propose to additionally quantize them into Int8 by the recent blockwize quantization method\cite{}. This method is shown to have low error rates being computationally efficient. Thus, this approach allows us to halve the network traffic without noticeable accuracy losses and computational costs.

\paragraph{Client-side routing.} Next, we want clients to be able to find a sequence of servers that run the model in the least amount of time. During generation, clients process one or few tokens at a time; in practice, the inference time is mostly sensitive to the network latency. Thus, clients have to ping nearby servers to measure latency and then find the path with minimal time via beam search. Conversely, during fine-tuning one needs to process a batch of examples in parallel. %Here, performance depends less on latency and more on bandwidth and compute.
Here, clients can split their batches between multiple servers using the algorithm from~\citet{ryabinin2021swarm}.
If a server fails during training or inference, a client removes it from consideration and reruns routing to find a replacement. During inference, the client sends all previous inputs to the replacement server, so that it has the same attention keys and values.


% routing algorithm provides a sequence of remote servers to visit, some of them may become unresponsive in the middle of an inference or training iteration. Therefore, we employ a recovery procedure able to efficiently restore the computations without disturbing the client. %In our system, we assume that such failures can appear <TODO> per <TODO> in average.

% Specifically, if the next server does not respond, the recovery procedure asks the routing algorithm to reconstruct the missing interval of the path. Once the path is restored, we are ready to continue routing from the breaking point. The inference and backward pass recovery requires an extra forward pass for newly arrived servers to restore past key-value caches and intermediate inputs respectively. Note that this operation engages only a few model blocks and has negligible amortized overhead.

% [The description of this feature was moved the "Server load balancing" section.]
% If there is no available server with appropriate blocks, the routing algorithm initiates a re-balancing procedure encouraging other servers to load missing blocks instead of the frequent ones.

%Below, we measure the performance of the proposed system in various setups. 

\begin{table}[t]
% \vspace{-14pt}
\centering
 \caption{Performance of sequential inference steps and training-time forward passes.}
 \vspace{-6px}
\label{tbl:experiments}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{6pt}
\begin{tabular}{cccccc}\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\bf{Network}}} & \multicolumn{2}{c}{\bf{Inference (steps/s)}} & \multicolumn{2}{c}{\bf{Forward (tokens/s)}}\\
\cmidrule{3-6}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\bf{Sequence length}} & \multicolumn{2}{c}{\bf{Batch size}}\\
\midrule
\bf{Bandwidth} & \bf{Latency} & 128 & 2048 & 1 & 64 \\\midrule
\multicolumn{6}{c}{Offloading, max. speed on 1x A100}\\
\midrule
256 Gbit/s &  --  &  0.18 & 0.18 & 2.7 & 170.3\\
128 Gbit/s &  --  &  0.09 & 0.09 & 2.4 & 152.8\\
\midrule
\multicolumn{6}{c}{Offloading, max. speed on 3x A100}\\
\midrule
256 Gbit/s &  --  &  0.09 & 0.09 & 5.1 & 325.1\\
128 Gbit/s &  --  &  0.05 & 0.05 & 3.5 & 226.3\\
\midrule
\multicolumn{6}{c}{\textsc{Petals} on 3 physical servers, with one A100 each}\\
\midrule
1 Gbit/s &  < 5 ms  &  1.22 & 1.11 & 70.0 & 253.6\\
100 Mbit/s  &  < 5 ms  &  1.19 & 1.08 & 56.4 & 182.0\\
100 Mbit/s  & 100 ms &  0.89 & 0.8 & 19.7 & 112.2\\
\midrule
\multicolumn{6}{c}{\textsc{Petals} on 12 virtual servers, simulated on 3x A100}\\
\midrule
1 Gbit/s  &  < 5 ms  &  0.97 & 0.86 & 37.9 & 180.0\\
100 Mbit/s  &  < 5 ms  &   0.97 & 0.86 & 25.6 & 66.6\\
100 Mbit/s  & 100 ms &   0.44 & 0.41 & 5.8  & 44.3\\
\midrule
\multicolumn{6}{c}{\textsc{Petals} on 14 real servers in Europe and North America}\\
\midrule
% \multicolumn{2}{c}{Real world}  &  0.63 & 0.57 & 28.3 & 135.4\\
\multicolumn{2}{c}{Real world}  &  0.68 & 0.61 & 32.6 & 179.4 \\
\bottomrule
\end{tabular}}
\vspace{-10pt}
\end{table}

% \begin{table}[]
% \centering
% \caption{Inference performance in token latency (seconds per token). The evaluation is performed in the local environment for various network configurations. The batch size is $1$.}
% \label{tbl:inference_local}
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{ccccc}\toprule
% \# Servers & BW (Mbps) & Ping (ms) & (Prefix, Sequence) & Token latency \\
% \midrule
% $3$  &  $1000$  &  $0$  &   ($1$, $128$)  &  $0.82$ \\
% $3$  &   $100$  &  $0$  &   ($1$, $128$)  &  $0.84$ \\
% $3$  &   $100$  & $100$ &   ($1$, $128$)  &  $1.13$ \\
% \midrule
% $12$ &  $1000$  &  $0$  &   ($1$, $128$)  &  $1.03$ \\
% $12$ &   $100$  &  $0$  &   ($1$, $128$)  &  $1.03$ \\
% $12$ &   $100$  & $100$ &   ($1$, $128$)  &  $2.26$ \\
% \midrule
% $3$  &  $1000$  &  $0$  &  ($1024$, $2048$)  & $0.90$ \\
% $3$  &   $100$  &  $0$  &  ($1024$, $2048$)  & $0.93$ \\
% $3$  &   $100$  & $100$ &  ($1024$, $2048$)  & $1.25$ \\
% \midrule
% $12$ &  $1000$  &  $0$  &  ($1024$, $2048$)  & $1.16$ \\
% $12$ &   $100$  &  $0$  &  ($1024$, $2048$)  & $1.16$ \\
% $12$ &   $100$  & $100$ &  ($1024$, $2048$)  & $2.43$ \\
% \bottomrule
% \end{tabular}}
% \end{table}
% \begin{table}[]
% \centering
% \caption{Parallel processing performance in throughput (tokens per second). The evaluation is performed in the local environment for various network configurations. Sequence length is $128$.}
% \label{tbl:parallel_local}
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{ccccc}\toprule
% \# Servers & BW (Mbps) & Ping (ms) & Batch & Throughput \\
% \midrule
% $3$  &  $1000$  &  $0$  &   $1$  & $70.0$ \\
% $3$  &   $100$  &  $0$  &   $1$  & $56.4$ \\
% $3$  &   $100$  & $100$ &   $1$  & $19.7$ \\
% \midrule
% $3$  &  $1000$  &  $0$  &  $64$  & $253.6$ \\
% $3$  &   $100$  &  $0$  &  $64$  & $182.0$ \\
% $3$  &   $100$  & $100$ &  $64$  & $112.2$ \\
% \midrule
% $12$ &  $1000$  &  $0$  &   $1$  & $37.9$ \\
% $12$ &   $100$  &  $0$  &   $1$  & $25.6$ \\
% $12$ &   $100$  & $100$ &   $1$  & $5.8$  \\
% \midrule
% $12$ &  $1000$  &  $0$  &  $64$  & $180.0$ \\
% $12$ &   $100$  &  $0$  &  $64$  & $66.6$  \\
% $12$ &   $100$  & $100$ &  $64$  & $44.3$  \\
% \bottomrule

\subsection{Benchmarks}

We evaluate the performance of \textsc{Petals} by running BLOOM-176B in emulated and real-world setups. Our first setup consists of 3 local servers, each running on an A100 80GB GPU. This is an optimistic scenario that requires the least amount of communication.
%First, we deploy $3$ servers on a local machine with 3xA100 80GB GPUs. Each server reserves one physical GPU. This setting represents a scenario when a client has minimum communication with servers.
In the second setup, we simulate 12 weaker devices by partitioning each A100-80GB into several virtual servers (3 large and 1 small). %nd require to have no servers with consequent blocks on a single physical GPU.
We evaluate the above setups with three network configurations: 1~Gbit/s with < 5 ms latency, 100 Mbit/s with < 5 ms latency and 100 Mbit/s with 100 ms latency\footnote{We simulate network conditions with \url{https://github.com/magnific0/wondershaper}, which uses \texttt{tc qdisc}}. The client-side nodes have 8 CPU cores and no GPU.

% Meanwhile, on the client side in the local environment to investigate the influence of bandwidth and latency on the overall performance. In particular, we evaluate three network settings: (a) 1 Gbit/s and (b) 100 Mbit/s with low latency and (c) 100 Mbit/s with 100 ms latency. In the external environment, we consider only initial client network limitations.

% Next, we benchmark BLOOM in a real-world distributed setting with 8 smaller servers holding RTX 3060, 3090, A4000, 8000, and 4$\times$2080Ti GPUs. These servers are split evenly between Europe and North America and connected to the Internet at speeds of 100--1000Mb/s. Since these GPUs cannot load the full BLOOM model, we load the first half of model layers and then repeat them twice. We verified that this results in a close estimate of the 176B model in the first two setups.
Next, we benchmark BLOOM in a real-world distributed setting with 14 smaller servers holding RTX 2$\times$3060, 4$\times$2080Ti, 2$\times$3090, 2$\times$A4000, and 4$\times$A5000 GPUs. These are personal servers and servers from university labs, spread across Europe and North America and connected to the Internet at speeds of 100--1000 Mbit/s. Four of the servers operate from under firewalls\footnote{We use the Circuit Relay protocol from libp2p to traverse NATs and firewalls, see \url{https://docs.libp2p.io/concepts/circuit-relay/}}.

In Table~\ref{tbl:experiments}, we report the performance of sequential inference and training-time forward passes. For inference, performance does not depend much on bandwidth or sequence length but degrades in high-latency settings, especially for 12 virtual servers.
In turn, training-time forward passes for large batches are affected by both bandwidth and latency.
%After examining the network traffic, we believe this is caused by TCP throughput degradation under latency.

We also test the effect of having multiple clients. For 12 servers with 100 Mbit/s bandwidth and 100 ms latency, if 8 clients run inference concurrently, each of them gets $\approx20\%$ slowdown compared to the case when it runs inference alone.
% 8 clients can run concurrent inference for $\approx20\%$ slowdown for each client.
%The performance on consumer hardware is comparable to the local environment setting with 12~virtual servers and a client with 100 Mbit/s bandwidth and 100 ms latency. %Moreover, the proposed system significantly outperforms the offloading results, presented in Table~\ref{tbl:memory_footprint}.

%The servers are equally distributed between two different continents. The machines within one continent are located in the neighborhood cities. A client is located in far away from both server locations. 


% \end{tabular}}
% \end{table}

Additionally, we compare \textsc{Petals} with parameter offloading to run large models with limited resources \citep{zerooffload,rajbhandari2021zero}. For the offloading benchmark we calculate the maximum inference and forward training throughput to receive an upper bound on offloading performance. We base our offloading numbers on the best possible hardware setup for offloading: CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes per GPU and PCIe switches for pairs of GPUs.

We calculate the maximum throughput for offloading as follows. In 8-bit, the model uses 1 GB of memory per billion parameters while PCIe~4.0 with 16 lanes has a throughput of 256 Gbit/s (or 128 Gbit/s if two GPUs are behind a PCIe switch). As such, offloading 176B parameters takes 5.5 seconds for a regular setup and 11 seconds for a multi-GPU setup. We assume an offloading latency of zero for the upper bound estimation.

These results are also shown in Table~\ref{tbl:experiments}. We can see that offloading is about an order of magnitude slower for inference compared to \textsc{Petals}. For the training-time forward pass, offloading is competitive if multiple GPUs are used and the networking for \textsc{Petals} is limited to 100 Mbit/s or has high latency. In other cases, \textsc{Petals} offers higher throughput than offloading for training.